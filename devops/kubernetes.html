<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-06-21 Fri 10:37 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Kubernetes</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Hao Ruan">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
<link href="../org-html-themes/solarized/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Kubernetes</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orge9b9f4d">1. Resources</a>
<ul>
<li><a href="#org981f283">1.1. Overview</a></li>
<li><a href="#org7b6470e">1.2. Pod</a>
<ul>
<li><a href="#orgcc2e5b3">1.2.1. YAML Definition</a></li>
<li><a href="#org30d8c11">1.2.2. Label</a></li>
<li><a href="#org8e1fa2e">1.2.3. Annotation</a></li>
<li><a href="#orgff5721c">1.2.4. Deleting</a></li>
<li><a href="#orgf33568e">1.2.5. Liveness probes</a></li>
<li><a href="#org9b0ff11">1.2.6. Init Containers</a></li>
<li><a href="#org4fb3170">1.2.7. Lifecycle hooks</a>
<ul>
<li><a href="#orgb515b02">1.2.7.1. Post-Start hook</a></li>
<li><a href="#orgd83f894">1.2.7.2. Pre-Stop hook</a></li>
</ul>
</li>
<li><a href="#orgec54fee">1.2.8. Shutdown</a></li>
</ul>
</li>
<li><a href="#org1ca183f">1.3. ReplicationController</a>
<ul>
<li><a href="#org62b0513">1.3.1. Three Parts Of A Replicationcontroller</a></li>
<li><a href="#orgbe914dc">1.3.2. Create</a></li>
<li><a href="#orgecfac6d">1.3.3. Change Pod Template</a></li>
<li><a href="#orgd8a84c9">1.3.4. Delete RC without deleting Pods</a></li>
</ul>
</li>
<li><a href="#org34a9197">1.4. ReplicaSet</a>
<ul>
<li><a href="#orga24f338">1.4.1. More Expressive Label Selectors</a></li>
<li><a href="#orgfd58380">1.4.2. Only cares about number of pods</a></li>
</ul>
</li>
<li><a href="#org84831a0">1.5. DaemonSet</a></li>
<li><a href="#org3ecdedc">1.6. Job</a>
<ul>
<li><a href="#org72396e7">1.6.1. Definition</a></li>
<li><a href="#org16dd420">1.6.2. Limit time for Job pod to complete</a></li>
</ul>
</li>
<li><a href="#org948c35b">1.7. CronJob</a></li>
<li><a href="#org1d579a5">1.8. Service</a>
<ul>
<li><a href="#org4cfaaa9">1.8.1. Overall</a></li>
<li><a href="#orgfb7e821">1.8.2. Creation</a>
<ul>
<li><a href="#org8081c57">1.8.2.1. By <code>kubectl expose</code></a></li>
<li><a href="#orgcd7df6e">1.8.2.2. By YAML</a>
<ul>
<li><a href="#org20dd7c7">1.8.2.2.1. Expose ports in same Service</a></li>
<li><a href="#orge594d4b">1.8.2.2.2. Using Named Ports</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6fede82">1.8.3. Session Affinity</a></li>
<li><a href="#org769331e">1.8.4. Discovering Service</a>
<ul>
<li><a href="#org5de9cad">1.8.4.1. Through ENV</a></li>
<li><a href="#org3152ab8">1.8.4.2. Through DNS</a></li>
</ul>
</li>
<li><a href="#org7a30639">1.8.5. Endpoints</a>
<ul>
<li><a href="#org7b803eb">1.8.5.1. Manually config service endpoints</a></li>
<li><a href="#org44796f7">1.8.5.2. Creat alias for external service</a></li>
</ul>
</li>
<li><a href="#orgf6724e7">1.8.6. Exposing Services To External Clients</a>
<ul>
<li><a href="#orgdf046ad">1.8.6.1. NodePort</a>
<ul>
<li><a href="#orgdd5e055">1.8.6.1.1. YAML Definition</a></li>
</ul>
</li>
<li><a href="#orgaf84383">1.8.6.2. LoadBalancer</a>
<ul>
<li><a href="#org699e3cb">1.8.6.2.1. YAML Definition</a></li>
</ul>
</li>
<li><a href="#orga3715a1">1.8.6.3. Ingress</a>
<ul>
<li><a href="#org3816ce4">1.8.6.3.1. YAML Definition</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdc5ecb8">1.8.7. Readiness Probe</a></li>
<li><a href="#orgd8de87e">1.8.8. Headless</a></li>
<li><a href="#orgdff01e4">1.8.9. External Traffic Issue</a>
<ul>
<li><a href="#org09c0e7c">1.8.9.1. Non-Preservation Of The Client's IP Issue</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc8fbabd">1.9. Volume</a>
<ul>
<li><a href="#org94a09be">1.9.1. Volume Types</a></li>
<li><a href="#org03ab9c1">1.9.2. emptyDir</a>
<ul>
<li><a href="#org82d0528">1.9.2.1. The medium used</a></li>
</ul>
</li>
<li><a href="#org7a6750b">1.9.3. hostPath</a></li>
<li><a href="#org09da27c">1.9.4. PV/PVC</a>
<ul>
<li><a href="#org4663291">1.9.4.1. Creating PersistentVolume</a>
<ul>
<li><a href="#orgc5191bd">1.9.4.1.1. Access Mode</a></li>
</ul>
</li>
<li><a href="#orge8e5aa9">1.9.4.2. Creating PersistentVolumeClaim</a></li>
<li><a href="#org09e87af">1.9.4.3. Using PersistentVolumeClaim in Pod</a></li>
<li><a href="#orgc106a31">1.9.4.4. Dynamic Provisioning of PV</a>
<ul>
<li><a href="#org4aaaca8">1.9.4.4.1. Defining StorageClass</a></li>
<li><a href="#org870382f">1.9.4.4.2. Requesting the SC in a PVC</a></li>
<li><a href="#org957ab41">1.9.4.4.3. Creating a PVC Without SC</a></li>
<li><a href="#orgddb26a7">1.9.4.4.4. Forcing a PVC to Be Bound to One of The Pre-Provisioned PVs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2a59b4e">1.10. ConfigMap &amp; Secret</a>
<ul>
<li><a href="#orgb7c84f0">1.10.1. Basic</a>
<ul>
<li><a href="#org009130c">1.10.1.1. Overriding the command and arguments</a></li>
<li><a href="#org7838b3d">1.10.1.2. Specifing env variables</a></li>
</ul>
</li>
<li><a href="#org34d6d8f">1.10.2. ConfigMap</a>
<ul>
<li><a href="#orgaba7703">1.10.2.1. Creating</a>
<ul>
<li><a href="#org0e731a7">1.10.2.1.1. CLI</a></li>
<li><a href="#orgbdb9b8b">1.10.2.1.2. YAML</a></li>
</ul>
</li>
<li><a href="#org80e78e2">1.10.2.2. Passing CM to Container</a>
<ul>
<li><a href="#orga25705c">1.10.2.2.1. As ENV</a></li>
<li><a href="#orgac017d9">1.10.2.2.2. As Volume</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga8fe4c7">1.10.3. Secret</a>
<ul>
<li><a href="#orge80196a">1.10.3.1. Default Token Secret</a></li>
<li><a href="#org4f8b3b6">1.10.3.2. Image Pull Secret</a></li>
<li><a href="#org7837679">1.10.3.3. Passing Secret to Container</a>
<ul>
<li><a href="#org0b9f597">1.10.3.3.1. As Volume</a></li>
<li><a href="#org94cfb8a">1.10.3.3.2. As ENV</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org382bc5f">1.11. Deployment</a>
<ul>
<li><a href="#org77fc172">1.11.1. Updating Strategies</a></li>
<li><a href="#org142808a">1.11.2. Performing RU with <code>kubectl</code></a>
<ul>
<li><a href="#org42f1dd6">1.11.2.1. Steps Required Before RU</a></li>
</ul>
</li>
<li><a href="#org57cf654">1.11.3. Creating Deployment</a></li>
<li><a href="#org294a429">1.11.4. Updating Deployment</a></li>
<li><a href="#org398dbb7">1.11.5. Rolling Back</a></li>
<li><a href="#orgedc4d3e">1.11.6. maxSurge/maxUnavailable</a></li>
<li><a href="#org70c90c1">1.11.7. Pausing Rollout Process</a></li>
<li><a href="#org161b30d">1.11.8. Blocking Rollout (minReadySeconds)</a></li>
</ul>
</li>
<li><a href="#org1edcb27">1.12. StatefulSet</a>
<ul>
<li><a href="#orga78315c">1.12.1. 设计思想</a>
<ul>
<li><a href="#org5360d99">1.12.1.1. Stable Identity</a>
<ul>
<li><a href="#org1b0c3ed">1.12.1.1.1. Governing Service</a></li>
<li><a href="#orgdeaa451">1.12.1.1.2. Scaling</a></li>
</ul>
</li>
<li><a href="#org4d43daa">1.12.1.2. Stable Dedicated Storage</a></li>
<li><a href="#org0d32194">1.12.1.3. Guarantees</a></li>
</ul>
</li>
<li><a href="#orgce3bfd5">1.12.2. Deploying App through StatefulSet</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org67e8ec2">2. Internals</a>
<ul>
<li><a href="#org8c01686">2.1. Architecture</a>
<ul>
<li><a href="#org444b5dd">2.1.1. etcd</a>
<ul>
<li><a href="#orga473ef2">2.1.1.1. Optimistic Concurrency Control</a></li>
<li><a href="#org4ed2d75">2.1.1.2. RAFT</a></li>
</ul>
</li>
<li><a href="#orgc88c80e">2.1.2. API Server</a>
<ul>
<li><a href="#orgc19be46">2.1.2.1. Resource Changes Notification</a></li>
</ul>
</li>
<li><a href="#orgc78a02e">2.1.3. Scheduler</a></li>
<li><a href="#org8491ee9">2.1.4. Controller Manager</a></li>
<li><a href="#org53d6e72">2.1.5. Kublet</a>
<ul>
<li><a href="#org40e706e">2.1.5.1. Run Static Pods without API Server</a></li>
</ul>
</li>
<li><a href="#orgea44939">2.1.6. Kube-Proxy</a></li>
<li><a href="#orgf22fbf4">2.1.7. Add-on</a></li>
</ul>
</li>
<li><a href="#orgb473c80">2.2. Cooperation between controllers</a></li>
<li><a href="#org42e6cf2">2.3. Pod</a>
<ul>
<li><a href="#orgb9bb8ad">2.3.1. Infrastructure Container</a></li>
<li><a href="#org92f59a3">2.3.2. Inter-pod networking</a>
<ul>
<li><a href="#orgda210e6">2.3.2.1. Pods are connected NAT-less</a></li>
<li><a href="#org48430c9">2.3.2.2. Communication between nodes</a></li>
<li><a href="#orge3f0964">2.3.2.3. Container Network Interface</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7a00fa5">2.4. High Availability</a></li>
</ul>
</li>
<li><a href="#orgdc6987c">3. Security</a>
<ul>
<li><a href="#orge291ad4">3.1. Understanding Authentication</a>
<ul>
<li><a href="#orgbaa7f03">3.1.1. Users</a></li>
<li><a href="#org6a7f9bc">3.1.2. Group</a></li>
<li><a href="#orgac7169a">3.1.3. Service Account</a>
<ul>
<li><a href="#org156db4a">3.1.3.1. Mountable Secrets</a></li>
<li><a href="#orge3324b2">3.1.3.2. Image Pull Secrets</a></li>
<li><a href="#org3577493">3.1.3.3. Assigning ServiceAccount to Pod</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7c5a809">3.2. RBAC</a>
<ul>
<li><a href="#org8aede0b">3.2.1. Role/RoleBinding</a>
<ul>
<li><a href="#org16139f1">3.2.1.1. Create Role</a></li>
<li><a href="#org377b2c0">3.2.1.2. Binding Role to ServiceAccount</a>
<ul>
<li><a href="#orgf34c60d">3.2.1.2.1. Including ServiceAccounts from other namespaces</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgba62632">3.2.2. ClusterRole/ClusterRoleBinding</a>
<ul>
<li><a href="#org7d23286">3.2.2.1. Access non-resource URLs</a></li>
<li><a href="#org1ba0bd2">3.2.2.2. Access namespaced resources</a></li>
</ul>
</li>
<li><a href="#org2e7e974">3.2.3. Combinations of role/binding types</a></li>
<li><a href="#org041a750">3.2.4. Default CRs/CRBs</a></li>
</ul>
</li>
<li><a href="#orgda71eee">3.3. Use Node's namespaces</a>
<ul>
<li><a href="#orgf9a791c">3.3.1. Use node’s network namespace</a>
<ul>
<li><a href="#org8bc4232">3.3.1.1. Use host port (not in host's nw ns)</a></li>
</ul>
</li>
<li><a href="#org68b5fff">3.3.2. Use node's PID/IPC namespaces</a></li>
</ul>
</li>
<li><a href="#org31523a4">3.4. Container security context</a>
<ul>
<li><a href="#org32937ff">3.4.1. Run container as specific user</a></li>
<li><a href="#org40b9435">3.4.2. Run container as non-root</a></li>
<li><a href="#org703a9d5">3.4.3. Run container in privileged mode</a></li>
<li><a href="#org75c583c">3.4.4. Add individual kernel capabilities</a></li>
<li><a href="#org56677fb">3.4.5. Drop individual kernel capabilities</a></li>
<li><a href="#orgad8249e">3.4.6. Prevent writing to filesystem</a></li>
<li><a href="#org6eca4b9">3.4.7. Share volume among different users</a></li>
</ul>
</li>
<li><a href="#org5bac8cf">3.5. PodSecurityPolicy</a></li>
<li><a href="#org4c997ea">3.6. Inter-Pod Network Isolation</a>
<ul>
<li><a href="#org211b7f8">3.6.1. Network isolation in a namespace</a></li>
<li><a href="#org3f54a20">3.6.2. Allow some pods in a ns to connect</a></li>
<li><a href="#orga26fac6">3.6.3. Isolate network between namespaces</a></li>
<li><a href="#org20c5e32">3.6.4. Isolate by CIDR notation</a></li>
<li><a href="#org5791726">3.6.5. Limite outbound traffic</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5141bee">4. Computation Resources</a>
<ul>
<li><a href="#org2f003d4">4.1. Request resources for containers</a>
<ul>
<li><a href="#orgb70ce97">4.1.1. Create pods with resource requests</a>
<ul>
<li><a href="#org348089a">4.1.1.1. How requests affect scheduling</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6ebc020">4.2. Limit resources for containers</a>
<ul>
<li><a href="#orgced9fff">4.2.1. Exceeding limits</a></li>
</ul>
</li>
<li><a href="#orgacdbb79">4.3. Pod QoS classes</a>
<ul>
<li><a href="#orgfc8c8fe">4.3.1. Kill Pod according to QoS</a></li>
</ul>
</li>
<li><a href="#org2f54c66">4.4. Set default requests/limits for pods per ns</a></li>
<li><a href="#org6d6568a">4.5. Limit total resources available in ns</a>
<ul>
<li><a href="#org60ab125">4.5.1. For CPU and Memory</a></li>
<li><a href="#orga19ba8f">4.5.2. For persistent storage</a></li>
<li><a href="#orgc8c5597">4.5.3. For number of objects</a></li>
<li><a href="#orgf4f9c23">4.5.4. For pod states and/or QoS classes</a></li>
</ul>
</li>
<li><a href="#org1d1661e">4.6. Resource usage Monitoring</a>
<ul>
<li><a href="#org51f8a5d">4.6.1. Analyzing resource usage with Grafana</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd9741fa">5. AutoScaling</a>
<ul>
<li><a href="#org8b00482">5.1. Horizontal pod autoscaling</a>
<ul>
<li><a href="#org5a10881">5.1.1. Autoscaling process</a>
<ul>
<li><a href="#org1b1cc1e">5.1.1.1. Whole picture</a></li>
<li><a href="#orge7c658e">5.1.1.2. CPU utilization</a></li>
<li><a href="#orgd169309">5.1.1.3. Maximum rate of scaling</a></li>
</ul>
</li>
<li><a href="#org6b4127e">5.1.2. Create HorizontalPodAutoscaler</a></li>
</ul>
</li>
<li><a href="#orgc613aac">5.2. Horizontal node autoscaling</a>
<ul>
<li><a href="#orgc242f25">5.2.1. ClusterAutoscaler</a>
<ul>
<li><a href="#org65454bb">5.2.1.1. Scaling up case</a></li>
<li><a href="#orgf672faf">5.2.1.2. Scaling down case</a></li>
</ul>
</li>
<li><a href="#org0c40904">5.2.2. Manually cordon and drain nodes</a></li>
<li><a href="#orgdeed022">5.2.3. PodDisruptionBudget</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org073c4a3">6. Scheduling</a>
<ul>
<li><a href="#org2c66667">6.1. Node Affinity</a>
<ul>
<li><a href="#org2d517cf">6.1.1. Hard</a></li>
<li><a href="#org835d7b5">6.1.2. Prioritizing</a></li>
</ul>
</li>
<li><a href="#org224ba28">6.2. Pod Affinity</a>
<ul>
<li><a href="#org14d1a3b">6.2.1. Hard</a></li>
<li><a href="#org8340090">6.2.2. Prioritizing</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdb4b2d9">7. Utils</a>
<ul>
<li><a href="#orgca2dc57">7.1. YAML Basic</a>
<ul>
<li><a href="#org491232b">7.1.1. Key Value Pair</a></li>
<li><a href="#org732e948">7.1.2. Array / List</a></li>
<li><a href="#orge69ff24">7.1.3. Dictionary / Map</a></li>
</ul>
</li>
<li><a href="#org6d6ba05">7.2. Bash completion</a></li>
<li><a href="#orgf8e79ab">7.3. Show all supported resources</a></li>
<li><a href="#orgf05f45f">7.4. Expose pod through Service</a></li>
<li><a href="#org40478e9">7.5. Run Temporary Pod</a></li>
<li><a href="#org17b4bf7">7.6. Discover possible API object fields</a></li>
<li><a href="#org5696165">7.7. Forward local network port to port in the pod</a></li>
<li><a href="#org73b2bfd">7.8. Communicate with Pods through API Server</a></li>
<li><a href="#orga017b52">7.9. Connect to Services through API Server</a></li>
<li><a href="#org0a6f98a">7.10. Obtain the log of crashed container</a></li>
<li><a href="#org386b631">7.11. Get IPs of all nodes</a></li>
<li><a href="#orga22c10a">7.12. Talking to API Server</a>
<ul>
<li><a href="#orgb37ceb1">7.12.1. kubectl proxy</a></li>
<li><a href="#orge0bdffc">7.12.2. From in POD</a></li>
</ul>
</li>
<li><a href="#orgdcf3f0d">7.13. Modify Deployments and other resources</a>
<ul>
<li><a href="#org83ea8e9">7.13.1. edit</a></li>
<li><a href="#org7ce5d8e">7.13.2. patch</a></li>
<li><a href="#org8538e4f">7.13.3. apply</a></li>
<li><a href="#org4857eaf">7.13.4. replace</a></li>
<li><a href="#org663b659">7.13.5. set image</a></li>
</ul>
</li>
<li><a href="#orgeeb1713">7.14. Check status of Control Plane components</a></li>
<li><a href="#orgfcf0933">7.15. WATCHING Resources</a></li>
<li><a href="#orge581778">7.16. Observing cluster events</a></li>
<li><a href="#orgded8ba7">7.17. Forcely shutdown pod</a></li>
<li><a href="#org24feee1">7.18. Copy file from/to container</a></li>
<li><a href="#org532765b">7.19. Mounting local files into the minikube vm</a></li>
<li><a href="#org50f3222">7.20. Using minikube docker daemon</a></li>
<li><a href="#org7d4d92a">7.21. Build images locally and load remotely</a></li>
<li><a href="#org6b3949f">7.22. <code>kubectl config</code></a>
<ul>
<li><a href="#org318d86a">7.22.1. Location of kubeconfig file</a></li>
<li><a href="#org3cc356f">7.22.2. Contents of kubeconfig file</a></li>
<li><a href="#org31568ea">7.22.3. Use cluster from local machine</a></li>
<li><a href="#org9828cbc">7.22.4. Name of current context</a></li>
<li><a href="#org8a5ff1b">7.22.5. Switch to different namespace</a></li>
<li><a href="#orged2800c">7.22.6. Switching between contexts</a></li>
<li><a href="#orgf89aacd">7.22.7. List contexts and clusters</a></li>
</ul>
</li>
<li><a href="#org78cb38f">7.23. Setup AWS EKS</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="meta">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left">Author</td>
<td class="org-left">Hao Ruan (haoru@cisco.com)</td>
</tr>

<tr>
<td class="org-left">Date</td>
<td class="org-left">2019-06-21 10:37:02</td>
</tr>
</tbody>
</table>
</div>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orge9b9f4d">1. Resources</a>
<ul>
<li><a href="#org981f283">1.1. Overview</a></li>
<li><a href="#org7b6470e">1.2. Pod</a>
<ul>
<li><a href="#orgcc2e5b3">1.2.1. YAML Definition</a></li>
<li><a href="#org30d8c11">1.2.2. Label</a></li>
<li><a href="#org8e1fa2e">1.2.3. Annotation</a></li>
<li><a href="#orgff5721c">1.2.4. Deleting</a></li>
<li><a href="#orgf33568e">1.2.5. Liveness probes</a></li>
<li><a href="#org9b0ff11">1.2.6. Init Containers</a></li>
<li><a href="#org4fb3170">1.2.7. Lifecycle hooks</a>
<ul>
<li><a href="#orgb515b02">1.2.7.1. Post-Start hook</a></li>
<li><a href="#orgd83f894">1.2.7.2. Pre-Stop hook</a></li>
</ul>
</li>
<li><a href="#orgec54fee">1.2.8. Shutdown</a></li>
</ul>
</li>
<li><a href="#org1ca183f">1.3. ReplicationController</a>
<ul>
<li><a href="#org62b0513">1.3.1. Three Parts Of A Replicationcontroller</a></li>
<li><a href="#orgbe914dc">1.3.2. Create</a></li>
<li><a href="#orgecfac6d">1.3.3. Change Pod Template</a></li>
<li><a href="#orgd8a84c9">1.3.4. Delete RC without deleting Pods</a></li>
</ul>
</li>
<li><a href="#org34a9197">1.4. ReplicaSet</a>
<ul>
<li><a href="#orga24f338">1.4.1. More Expressive Label Selectors</a></li>
<li><a href="#orgfd58380">1.4.2. Only cares about number of pods</a></li>
</ul>
</li>
<li><a href="#org84831a0">1.5. DaemonSet</a></li>
<li><a href="#org3ecdedc">1.6. Job</a>
<ul>
<li><a href="#org72396e7">1.6.1. Definition</a></li>
<li><a href="#org16dd420">1.6.2. Limit time for Job pod to complete</a></li>
</ul>
</li>
<li><a href="#org948c35b">1.7. CronJob</a></li>
<li><a href="#org1d579a5">1.8. Service</a>
<ul>
<li><a href="#org4cfaaa9">1.8.1. Overall</a></li>
<li><a href="#orgfb7e821">1.8.2. Creation</a>
<ul>
<li><a href="#org8081c57">1.8.2.1. By <code>kubectl expose</code></a></li>
<li><a href="#orgcd7df6e">1.8.2.2. By YAML</a>
<ul>
<li><a href="#org20dd7c7">1.8.2.2.1. Expose ports in same Service</a></li>
<li><a href="#orge594d4b">1.8.2.2.2. Using Named Ports</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6fede82">1.8.3. Session Affinity</a></li>
<li><a href="#org769331e">1.8.4. Discovering Service</a>
<ul>
<li><a href="#org5de9cad">1.8.4.1. Through ENV</a></li>
<li><a href="#org3152ab8">1.8.4.2. Through DNS</a></li>
</ul>
</li>
<li><a href="#org7a30639">1.8.5. Endpoints</a>
<ul>
<li><a href="#org7b803eb">1.8.5.1. Manually config service endpoints</a></li>
<li><a href="#org44796f7">1.8.5.2. Creat alias for external service</a></li>
</ul>
</li>
<li><a href="#orgf6724e7">1.8.6. Exposing Services To External Clients</a>
<ul>
<li><a href="#orgdf046ad">1.8.6.1. NodePort</a>
<ul>
<li><a href="#orgdd5e055">1.8.6.1.1. YAML Definition</a></li>
</ul>
</li>
<li><a href="#orgaf84383">1.8.6.2. LoadBalancer</a>
<ul>
<li><a href="#org699e3cb">1.8.6.2.1. YAML Definition</a></li>
</ul>
</li>
<li><a href="#orga3715a1">1.8.6.3. Ingress</a>
<ul>
<li><a href="#org3816ce4">1.8.6.3.1. YAML Definition</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdc5ecb8">1.8.7. Readiness Probe</a></li>
<li><a href="#orgd8de87e">1.8.8. Headless</a></li>
<li><a href="#orgdff01e4">1.8.9. External Traffic Issue</a>
<ul>
<li><a href="#org09c0e7c">1.8.9.1. Non-Preservation Of The Client's IP Issue</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc8fbabd">1.9. Volume</a>
<ul>
<li><a href="#org94a09be">1.9.1. Volume Types</a></li>
<li><a href="#org03ab9c1">1.9.2. emptyDir</a>
<ul>
<li><a href="#org82d0528">1.9.2.1. The medium used</a></li>
</ul>
</li>
<li><a href="#org7a6750b">1.9.3. hostPath</a></li>
<li><a href="#org09da27c">1.9.4. PV/PVC</a>
<ul>
<li><a href="#org4663291">1.9.4.1. Creating PersistentVolume</a>
<ul>
<li><a href="#orgc5191bd">1.9.4.1.1. Access Mode</a></li>
</ul>
</li>
<li><a href="#orge8e5aa9">1.9.4.2. Creating PersistentVolumeClaim</a></li>
<li><a href="#org09e87af">1.9.4.3. Using PersistentVolumeClaim in Pod</a></li>
<li><a href="#orgc106a31">1.9.4.4. Dynamic Provisioning of PV</a>
<ul>
<li><a href="#org4aaaca8">1.9.4.4.1. Defining StorageClass</a></li>
<li><a href="#org870382f">1.9.4.4.2. Requesting the SC in a PVC</a></li>
<li><a href="#org957ab41">1.9.4.4.3. Creating a PVC Without SC</a></li>
<li><a href="#orgddb26a7">1.9.4.4.4. Forcing a PVC to Be Bound to One of The Pre-Provisioned PVs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2a59b4e">1.10. ConfigMap &amp; Secret</a>
<ul>
<li><a href="#orgb7c84f0">1.10.1. Basic</a>
<ul>
<li><a href="#org009130c">1.10.1.1. Overriding the command and arguments</a></li>
<li><a href="#org7838b3d">1.10.1.2. Specifing env variables</a></li>
</ul>
</li>
<li><a href="#org34d6d8f">1.10.2. ConfigMap</a>
<ul>
<li><a href="#orgaba7703">1.10.2.1. Creating</a>
<ul>
<li><a href="#org0e731a7">1.10.2.1.1. CLI</a></li>
<li><a href="#orgbdb9b8b">1.10.2.1.2. YAML</a></li>
</ul>
</li>
<li><a href="#org80e78e2">1.10.2.2. Passing CM to Container</a>
<ul>
<li><a href="#orga25705c">1.10.2.2.1. As ENV</a></li>
<li><a href="#orgac017d9">1.10.2.2.2. As Volume</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga8fe4c7">1.10.3. Secret</a>
<ul>
<li><a href="#orge80196a">1.10.3.1. Default Token Secret</a></li>
<li><a href="#org4f8b3b6">1.10.3.2. Image Pull Secret</a></li>
<li><a href="#org7837679">1.10.3.3. Passing Secret to Container</a>
<ul>
<li><a href="#org0b9f597">1.10.3.3.1. As Volume</a></li>
<li><a href="#org94cfb8a">1.10.3.3.2. As ENV</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org382bc5f">1.11. Deployment</a>
<ul>
<li><a href="#org77fc172">1.11.1. Updating Strategies</a></li>
<li><a href="#org142808a">1.11.2. Performing RU with <code>kubectl</code></a>
<ul>
<li><a href="#org42f1dd6">1.11.2.1. Steps Required Before RU</a></li>
</ul>
</li>
<li><a href="#org57cf654">1.11.3. Creating Deployment</a></li>
<li><a href="#org294a429">1.11.4. Updating Deployment</a></li>
<li><a href="#org398dbb7">1.11.5. Rolling Back</a></li>
<li><a href="#orgedc4d3e">1.11.6. maxSurge/maxUnavailable</a></li>
<li><a href="#org70c90c1">1.11.7. Pausing Rollout Process</a></li>
<li><a href="#org161b30d">1.11.8. Blocking Rollout (minReadySeconds)</a></li>
</ul>
</li>
<li><a href="#org1edcb27">1.12. StatefulSet</a>
<ul>
<li><a href="#orga78315c">1.12.1. 设计思想</a>
<ul>
<li><a href="#org5360d99">1.12.1.1. Stable Identity</a>
<ul>
<li><a href="#org1b0c3ed">1.12.1.1.1. Governing Service</a></li>
<li><a href="#orgdeaa451">1.12.1.1.2. Scaling</a></li>
</ul>
</li>
<li><a href="#org4d43daa">1.12.1.2. Stable Dedicated Storage</a></li>
<li><a href="#org0d32194">1.12.1.3. Guarantees</a></li>
</ul>
</li>
<li><a href="#orgce3bfd5">1.12.2. Deploying App through StatefulSet</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org67e8ec2">2. Internals</a>
<ul>
<li><a href="#org8c01686">2.1. Architecture</a>
<ul>
<li><a href="#org444b5dd">2.1.1. etcd</a>
<ul>
<li><a href="#orga473ef2">2.1.1.1. Optimistic Concurrency Control</a></li>
<li><a href="#org4ed2d75">2.1.1.2. RAFT</a></li>
</ul>
</li>
<li><a href="#orgc88c80e">2.1.2. API Server</a>
<ul>
<li><a href="#orgc19be46">2.1.2.1. Resource Changes Notification</a></li>
</ul>
</li>
<li><a href="#orgc78a02e">2.1.3. Scheduler</a></li>
<li><a href="#org8491ee9">2.1.4. Controller Manager</a></li>
<li><a href="#org53d6e72">2.1.5. Kublet</a>
<ul>
<li><a href="#org40e706e">2.1.5.1. Run Static Pods without API Server</a></li>
</ul>
</li>
<li><a href="#orgea44939">2.1.6. Kube-Proxy</a></li>
<li><a href="#orgf22fbf4">2.1.7. Add-on</a></li>
</ul>
</li>
<li><a href="#orgb473c80">2.2. Cooperation between controllers</a></li>
<li><a href="#org42e6cf2">2.3. Pod</a>
<ul>
<li><a href="#orgb9bb8ad">2.3.1. Infrastructure Container</a></li>
<li><a href="#org92f59a3">2.3.2. Inter-pod networking</a>
<ul>
<li><a href="#orgda210e6">2.3.2.1. Pods are connected NAT-less</a></li>
<li><a href="#org48430c9">2.3.2.2. Communication between nodes</a></li>
<li><a href="#orge3f0964">2.3.2.3. Container Network Interface</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7a00fa5">2.4. High Availability</a></li>
</ul>
</li>
<li><a href="#orgdc6987c">3. Security</a>
<ul>
<li><a href="#orge291ad4">3.1. Understanding Authentication</a>
<ul>
<li><a href="#orgbaa7f03">3.1.1. Users</a></li>
<li><a href="#org6a7f9bc">3.1.2. Group</a></li>
<li><a href="#orgac7169a">3.1.3. Service Account</a>
<ul>
<li><a href="#org156db4a">3.1.3.1. Mountable Secrets</a></li>
<li><a href="#orge3324b2">3.1.3.2. Image Pull Secrets</a></li>
<li><a href="#org3577493">3.1.3.3. Assigning ServiceAccount to Pod</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7c5a809">3.2. RBAC</a>
<ul>
<li><a href="#org8aede0b">3.2.1. Role/RoleBinding</a>
<ul>
<li><a href="#org16139f1">3.2.1.1. Create Role</a></li>
<li><a href="#org377b2c0">3.2.1.2. Binding Role to ServiceAccount</a>
<ul>
<li><a href="#orgf34c60d">3.2.1.2.1. Including ServiceAccounts from other namespaces</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgba62632">3.2.2. ClusterRole/ClusterRoleBinding</a>
<ul>
<li><a href="#org7d23286">3.2.2.1. Access non-resource URLs</a></li>
<li><a href="#org1ba0bd2">3.2.2.2. Access namespaced resources</a></li>
</ul>
</li>
<li><a href="#org2e7e974">3.2.3. Combinations of role/binding types</a></li>
<li><a href="#org041a750">3.2.4. Default CRs/CRBs</a></li>
</ul>
</li>
<li><a href="#orgda71eee">3.3. Use Node's namespaces</a>
<ul>
<li><a href="#orgf9a791c">3.3.1. Use node’s network namespace</a>
<ul>
<li><a href="#org8bc4232">3.3.1.1. Use host port (not in host's nw ns)</a></li>
</ul>
</li>
<li><a href="#org68b5fff">3.3.2. Use node's PID/IPC namespaces</a></li>
</ul>
</li>
<li><a href="#org31523a4">3.4. Container security context</a>
<ul>
<li><a href="#org32937ff">3.4.1. Run container as specific user</a></li>
<li><a href="#org40b9435">3.4.2. Run container as non-root</a></li>
<li><a href="#org703a9d5">3.4.3. Run container in privileged mode</a></li>
<li><a href="#org75c583c">3.4.4. Add individual kernel capabilities</a></li>
<li><a href="#org56677fb">3.4.5. Drop individual kernel capabilities</a></li>
<li><a href="#orgad8249e">3.4.6. Prevent writing to filesystem</a></li>
<li><a href="#org6eca4b9">3.4.7. Share volume among different users</a></li>
</ul>
</li>
<li><a href="#org5bac8cf">3.5. PodSecurityPolicy</a></li>
<li><a href="#org4c997ea">3.6. Inter-Pod Network Isolation</a>
<ul>
<li><a href="#org211b7f8">3.6.1. Network isolation in a namespace</a></li>
<li><a href="#org3f54a20">3.6.2. Allow some pods in a ns to connect</a></li>
<li><a href="#orga26fac6">3.6.3. Isolate network between namespaces</a></li>
<li><a href="#org20c5e32">3.6.4. Isolate by CIDR notation</a></li>
<li><a href="#org5791726">3.6.5. Limite outbound traffic</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5141bee">4. Computation Resources</a>
<ul>
<li><a href="#org2f003d4">4.1. Request resources for containers</a>
<ul>
<li><a href="#orgb70ce97">4.1.1. Create pods with resource requests</a>
<ul>
<li><a href="#org348089a">4.1.1.1. How requests affect scheduling</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6ebc020">4.2. Limit resources for containers</a>
<ul>
<li><a href="#orgced9fff">4.2.1. Exceeding limits</a></li>
</ul>
</li>
<li><a href="#orgacdbb79">4.3. Pod QoS classes</a>
<ul>
<li><a href="#orgfc8c8fe">4.3.1. Kill Pod according to QoS</a></li>
</ul>
</li>
<li><a href="#org2f54c66">4.4. Set default requests/limits for pods per ns</a></li>
<li><a href="#org6d6568a">4.5. Limit total resources available in ns</a>
<ul>
<li><a href="#org60ab125">4.5.1. For CPU and Memory</a></li>
<li><a href="#orga19ba8f">4.5.2. For persistent storage</a></li>
<li><a href="#orgc8c5597">4.5.3. For number of objects</a></li>
<li><a href="#orgf4f9c23">4.5.4. For pod states and/or QoS classes</a></li>
</ul>
</li>
<li><a href="#org1d1661e">4.6. Resource usage Monitoring</a>
<ul>
<li><a href="#org51f8a5d">4.6.1. Analyzing resource usage with Grafana</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd9741fa">5. AutoScaling</a>
<ul>
<li><a href="#org8b00482">5.1. Horizontal pod autoscaling</a>
<ul>
<li><a href="#org5a10881">5.1.1. Autoscaling process</a>
<ul>
<li><a href="#org1b1cc1e">5.1.1.1. Whole picture</a></li>
<li><a href="#orge7c658e">5.1.1.2. CPU utilization</a></li>
<li><a href="#orgd169309">5.1.1.3. Maximum rate of scaling</a></li>
</ul>
</li>
<li><a href="#org6b4127e">5.1.2. Create HorizontalPodAutoscaler</a></li>
</ul>
</li>
<li><a href="#orgc613aac">5.2. Horizontal node autoscaling</a>
<ul>
<li><a href="#orgc242f25">5.2.1. ClusterAutoscaler</a>
<ul>
<li><a href="#org65454bb">5.2.1.1. Scaling up case</a></li>
<li><a href="#orgf672faf">5.2.1.2. Scaling down case</a></li>
</ul>
</li>
<li><a href="#org0c40904">5.2.2. Manually cordon and drain nodes</a></li>
<li><a href="#orgdeed022">5.2.3. PodDisruptionBudget</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org073c4a3">6. Scheduling</a>
<ul>
<li><a href="#org2c66667">6.1. Node Affinity</a>
<ul>
<li><a href="#org2d517cf">6.1.1. Hard</a></li>
<li><a href="#org835d7b5">6.1.2. Prioritizing</a></li>
</ul>
</li>
<li><a href="#org224ba28">6.2. Pod Affinity</a>
<ul>
<li><a href="#org14d1a3b">6.2.1. Hard</a></li>
<li><a href="#org8340090">6.2.2. Prioritizing</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdb4b2d9">7. Utils</a>
<ul>
<li><a href="#orgca2dc57">7.1. YAML Basic</a>
<ul>
<li><a href="#org491232b">7.1.1. Key Value Pair</a></li>
<li><a href="#org732e948">7.1.2. Array / List</a></li>
<li><a href="#orge69ff24">7.1.3. Dictionary / Map</a></li>
</ul>
</li>
<li><a href="#org6d6ba05">7.2. Bash completion</a></li>
<li><a href="#orgf8e79ab">7.3. Show all supported resources</a></li>
<li><a href="#orgf05f45f">7.4. Expose pod through Service</a></li>
<li><a href="#org40478e9">7.5. Run Temporary Pod</a></li>
<li><a href="#org17b4bf7">7.6. Discover possible API object fields</a></li>
<li><a href="#org5696165">7.7. Forward local network port to port in the pod</a></li>
<li><a href="#org73b2bfd">7.8. Communicate with Pods through API Server</a></li>
<li><a href="#orga017b52">7.9. Connect to Services through API Server</a></li>
<li><a href="#org0a6f98a">7.10. Obtain the log of crashed container</a></li>
<li><a href="#org386b631">7.11. Get IPs of all nodes</a></li>
<li><a href="#orga22c10a">7.12. Talking to API Server</a>
<ul>
<li><a href="#orgb37ceb1">7.12.1. kubectl proxy</a></li>
<li><a href="#orge0bdffc">7.12.2. From in POD</a></li>
</ul>
</li>
<li><a href="#orgdcf3f0d">7.13. Modify Deployments and other resources</a>
<ul>
<li><a href="#org83ea8e9">7.13.1. edit</a></li>
<li><a href="#org7ce5d8e">7.13.2. patch</a></li>
<li><a href="#org8538e4f">7.13.3. apply</a></li>
<li><a href="#org4857eaf">7.13.4. replace</a></li>
<li><a href="#org663b659">7.13.5. set image</a></li>
</ul>
</li>
<li><a href="#orgeeb1713">7.14. Check status of Control Plane components</a></li>
<li><a href="#orgfcf0933">7.15. WATCHING Resources</a></li>
<li><a href="#orge581778">7.16. Observing cluster events</a></li>
<li><a href="#orgded8ba7">7.17. Forcely shutdown pod</a></li>
<li><a href="#org24feee1">7.18. Copy file from/to container</a></li>
<li><a href="#org532765b">7.19. Mounting local files into the minikube vm</a></li>
<li><a href="#org50f3222">7.20. Using minikube docker daemon</a></li>
<li><a href="#org7d4d92a">7.21. Build images locally and load remotely</a></li>
<li><a href="#org6b3949f">7.22. <code>kubectl config</code></a>
<ul>
<li><a href="#org318d86a">7.22.1. Location of kubeconfig file</a></li>
<li><a href="#org3cc356f">7.22.2. Contents of kubeconfig file</a></li>
<li><a href="#org31568ea">7.22.3. Use cluster from local machine</a></li>
<li><a href="#org9828cbc">7.22.4. Name of current context</a></li>
<li><a href="#org8a5ff1b">7.22.5. Switch to different namespace</a></li>
<li><a href="#orged2800c">7.22.6. Switching between contexts</a></li>
<li><a href="#orgf89aacd">7.22.7. List contexts and clusters</a></li>
</ul>
</li>
<li><a href="#org78cb38f">7.23. Setup AWS EKS</a></li>
</ul>
</li>
</ul>
</div>
</div>



<div id="outline-container-orge9b9f4d" class="outline-2">
<h2 id="orge9b9f4d"><span class="section-number-2">1</span> Resources</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org981f283" class="outline-3">
<h3 id="org981f283"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">

<div class="figure">
<p><img src="img/k8s_whole_pic.png" alt="k8s_whole_pic.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org7b6470e" class="outline-3">
<h3 id="org7b6470e"><span class="section-number-3">1.2</span> Pod</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-orgcc2e5b3" class="outline-4">
<h4 id="orgcc2e5b3"><span class="section-number-4">1.2.1</span> YAML Definition</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
定义文件主要包含：
</p>

<ol class="org-ol">
<li>API version</li>
<li>Resource type</li>
<li>Metadata <br>
Includes the name, namespace, labels, and other information about the pod.</li>
<li>Spec <br>
Contains the actual description of the pod’s contents, such as the pod’s con- tainers, volumes, and other data.</li>
<li>Status<br>
Contains read-only runtime data that shows the state of the resource at a given moment.<br>
<b>When creating a new pod, you never need to provide the status part.</b></li>
</ol>


<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080       # the port the app is listening on
      protocol: TCP
</pre>
</div>


<pre class="example">
  在 POD 定义文件中定义 ports 是可选的行为，端口始终可以被访问，主要是为了方便用户清楚的了解提供了哪些服务。
</pre>
</div>
</div>



<div id="outline-container-org30d8c11" class="outline-4">
<h4 id="org30d8c11"><span class="section-number-4">1.2.2</span> Label</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
A label is an arbitrary key-value pair you attach to a resource, <br>
which is then utilized when selecting resources using <b>label selectors</b> <br>
(resources are filtered based on whether they include the label specified in the selector).
</p>


<div class="figure">
<p><img src="img/k8s_labels.png" alt="k8s_labels.png">
</p>
</div>

<p>
A label selector can select resources based on whether the resource:
</p>

<ol class="org-ol">
<li>Contains (or doesn’t contain) a label with a certain key <br></li>
<li>Contains a label with a certain key and value <br></li>
<li>Contains a label with a certain key, but with a value not equal to the one you specify</li>
</ol>
</div>
</div>


<div id="outline-container-org8e1fa2e" class="outline-4">
<h4 id="org8e1fa2e"><span class="section-number-4">1.2.3</span> Annotation</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
Annotations are also key-value pairs, so in essence, they’re similar to labels,
but they aren’t meant to hold identifying information.
</p>

<p>
They can’t be used to group objects the way labels can.
While objects can be selected through label selectors, <b>there’s no such thing as an annotation selector.</b>
</p>

<p>
Annotations can hold much larger pieces of information (up to 256 KB in total) and are primarily <b>meant to be used by tools.</b>
</p>

<pre class="example">
Annotations are also commonly used when introducing new features to Kubernetes.
Usually, alpha and beta versions of new features don’t introduce any new fields to API objects.
Annotations are used instead of fields, and then once the required API changes have become clear and been agreed upon by the Kubernetes developers, new fields are introduced and the related annotations deprecated.

A great use of annotations is adding descriptions for each pod or other API object, so that everyone using the cluster can quickly look up information about each individual object.
For example, an annotation used to specify the name of the person who created the object can make collaboration between everyone working on the cluster much easier.
</pre>
</div>
</div>


<div id="outline-container-orgff5721c" class="outline-4">
<h4 id="orgff5721c"><span class="section-number-4">1.2.4</span> Deleting</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are part of that pod.
</p>

<p>
Kubernetes sends a <b>SIGTERM</b> signal to the process and waits a certain number of seconds (30 by default) for it to shut down gracefully.
</p>

<p>
If it doesn’t shut down in time, the process is then killed through <b>SIGKILL</b>.
To make sure your processes are always shut down gracefully, they need to handle the SIGTERM signal properly.
</p>
</div>
</div>


<div id="outline-container-orgf33568e" class="outline-4">
<h4 id="orgf33568e"><span class="section-number-4">1.2.5</span> Liveness probes</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
As soon as a pod is scheduled to a node, the Kubelet on that node will run its containers and, from then on, <b>keep them running as long as the pod exists.</b>
</p>

<p>
If the container’s main process crashes, <b>the Kubelet will restart the container.</b>
</p>

<p>
Kubernetes can check if a <b>container</b> is still alive through <code>liveness probes</code>.
</p>

<p>
You can specify a liveness probe for each container in the pod’s specification.
Kubernetes will periodically execute the probe and restart the container if the probe fails.
</p>

<pre class="example">
Kubernetes also supports readiness probes. Be sure not to confuse the two. They’re used for two different things.
</pre>



<div class="figure">
<p><img src="img/k8s_pod_liveness.png" alt="k8s_pod_liveness.png">
</p>
</div>

<pre class="example">
Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3

The delay=0s part shows that the probing begins immediately after the container is started.
If you don’t set the initial delay, the prober will start probing the container as soon as it starts,
which usually leads to the probe failing, because the app isn’t ready to start receiving requests.

The timeout is set to only 1 second, so the container must return a response in 1 second or the probe is counted as failed.

The container is probed every 10 seconds (period=10s) and the container is restarted after the probe fails three consecutive times (#failure=3).
</pre>
</div>
</div>




<div id="outline-container-org9b0ff11" class="outline-4">
<h4 id="org9b0ff11"><span class="section-number-4">1.2.6</span> Init Containers</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
Pods can also include <b>init containers</b>. Tthey can be used to initialize the pod.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune-client
spec:
  initContainers:               # defining an init container, not a regular container.
  - name: init
    image: busybox
    command:
    - sh
    - -c
    - 'while true; do echo "Waiting for fortune service to come up..."; wget http://fortune -q -T 1 -O /dev/null &gt;/dev/null 2&gt;/dev/null &amp;&amp; break; sleep 1; done; echo "Service is up! Starting main container."' # The init container runs a loop that runs until the fortune Service is up.
  containers:
  - image: busybox
    name: main
    command:
    - sh
    - -c
    - 'echo "Main container started. Reading fortune very 10 seconds."; while true; do echo "-------------"; wget -q -O - http://fortune; sleep 10; done'
</pre>
</div>
</div>
</div>


<div id="outline-container-org4fb3170" class="outline-4">
<h4 id="org4fb3170"><span class="section-number-4">1.2.7</span> Lifecycle hooks</h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
Lifecycle hooks are specified <b>per container</b>, unlike init containers, which apply to the whole pod. <br>
They're executed when the container starts and before it stops.
</p>

<p>
Lifecycle hooks are similar to liveness and readiness probes in that they can either:
</p>

<ul class="org-ul">
<li>Execute a command inside the container</li>
<li>Perform an HTTP GET request against a URL</li>
</ul>


<pre class="example">
Lifecycle hooks target containers, NOT PODS.
</pre>
</div>

<div id="outline-container-orgb515b02" class="outline-5">
<h5 id="orgb515b02"><span class="section-number-5">1.2.7.1</span> Post-Start hook</h5>
<div class="outline-text-5" id="text-1-2-7-1">
<p>
A post-start hook is executed immediately after the container's main process is started.
Post-start hooks allow you to run additional commands without having to touch the app.
</p>

<p>
Until the hook completes, the container will stay in the <code>Waiting</code> state with the reason <code>ContainerCreating</code>. <br>
The pod's status will be <code>Pending</code> instead of <code>Running</code>. <br>
If the hook fails to run or returns a non-zero exit code, the main container will be killed.
</p>
</div>
</div>


<div id="outline-container-orgd83f894" class="outline-5">
<h5 id="orgd83f894"><span class="section-number-5">1.2.7.2</span> Pre-Stop hook</h5>
<div class="outline-text-5" id="text-1-2-7-2">
<p>
A pre-stop hook is executed immediately before a container is terminated.
</p>

<p>
Container will be terminated regardless of the result of the hook
(error HTTP response code or non-zero exit code when using a command-based hook)
will not prevent the container from being terminated.
</p>
</div>
</div>
</div>



<div id="outline-container-orgec54fee" class="outline-4">
<h4 id="orgec54fee"><span class="section-number-4">1.2.8</span> Shutdown</h4>
<div class="outline-text-4" id="text-1-2-8">
<p>
Then the following sequence of events is performed when pod is shutdown:
</p>

<ol class="org-ol">
<li>Run the pre-stop hook, if one is configured, and wait for it to finish.</li>
<li>Send the SIGTERM signal to the main process of the container.</li>
<li>Wait until the container shuts down cleanly or until the termination grace period runs out.</li>
<li>Forcibly kill the process with SIGKILL, if it hasn't terminated gracefully yet.</li>
</ol>



<div class="figure">
<p><img src="img/k8s_pod_shutdown.png" alt="k8s_pod_shutdown.png">
</p>
</div>


<p>
You can force the API server to delete the resource immediately, without waiting for confirmation,
by setting the grace period to zero and adding the <code>--force</code> option like this:
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl delete po mypod --grace-period=0 --force
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org1ca183f" class="outline-3">
<h3 id="org1ca183f"><span class="section-number-3">1.3</span> ReplicationController</h3>
<div class="outline-text-3" id="text-1-3">
<p>
A ReplicationController’s job is to make sure that an exact number of pods always <b>matches its label selector.</b> <br>
If it doesn’t, the ReplicationController takes the appropriate action to reconcile the actual with the desired number.
</p>


<div class="figure">
<p><img src="img/k8s_rc_loop.png" alt="k8s_rc_loop.png">
</p>
</div>
</div>


<div id="outline-container-org62b0513" class="outline-4">
<h4 id="org62b0513"><span class="section-number-4">1.3.1</span> Three Parts Of A Replicationcontroller</h4>
<div class="outline-text-4" id="text-1-3-1">

<div class="figure">
<p><img src="img/k8s_rc_3parts.png" alt="k8s_rc_3parts.png">
</p>
</div>


<p>
A ReplicationController’s replica count, the label selector, and even the pod template can all be modified at any time,
<b>but only changes to the replica count affect existing pods.</b>
</p>
</div>
</div>


<div id="outline-container-orgbe914dc" class="outline-4">
<h4 id="orgbe914dc"><span class="section-number-4">1.3.2</span> Create</h4>
<div class="outline-text-4" id="text-1-3-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia                  # what pods the RC is operating on
  template:                     # pod templat for creating new pod
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
</pre>
</div>



<p>
The pod labels in the template must obviously match the label selector of the ReplicationController;
otherwise the controller would create new pods indefinitely, because spinning up a new pod wouldn’t bring the actual replica count any closer to the desired number of replicas.
</p>

<p>
To prevent such scenarios, the API server verifies the ReplicationController definition and will not accept it if it’s misconfigured.
<b>Not specifying the selector at all is also an option.</b> In that case, it will be configured automatically from the labels in the pod template.
</p>


<pre class="example">
Don’t specify a pod selector when defining a ReplicationController.
Let Kubernetes extract it from the pod template. This will keep your YAML shorter and simpler.
</pre>

<p>
Although a pod isn’t tied to a ReplicationController, the pod does reference it in the <code>metadata.ownerReferences</code> field, <br>
which you can use to easily find which ReplicationController a pod belongs to.
</p>
</div>
</div>


<div id="outline-container-orgecfac6d" class="outline-4">
<h4 id="orgecfac6d"><span class="section-number-4">1.3.3</span> Change Pod Template</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
A ReplicationController’s pod template can be modified at any time. <br>
Changing the pod template is like replacing a cookie cutter with another one. <br>
It will only affect the cookies you cut out afterward and will have no effect on the ones you’ve already cut. <br>
To modify the old pods, you’d need to delete them and let the ReplicationController replace them with new ones based on the new template.
</p>


<div class="figure">
<p><img src="img/k8s_rc_change_pod_tmpl.png" alt="k8s_rc_change_pod_tmpl.png">
</p>
</div>
</div>
</div>


<div id="outline-container-orgd8a84c9" class="outline-4">
<h4 id="orgd8a84c9"><span class="section-number-4">1.3.4</span> Delete RC without deleting Pods</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
When you delete a ReplicationController through kubectl delete, the pods are also deleted.<br>
But because pods created by a ReplicationController aren’t an integral part of the ReplicationController, and are only managed by it, <br>
<b>you can delete only the ReplicationController and leave the pods running:</b>
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl delete rc &lt;rc-name&gt; --cascade=false
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org34a9197" class="outline-3">
<h3 id="org34a9197"><span class="section-number-3">1.4</span> ReplicaSet</h3>
<div class="outline-text-3" id="text-1-4">
<p>
You usually won’t create them directly, but instead have them created automatically <b>when you create the higher-level Deployment resource.</b>
</p>

<p>
A ReplicaSet behaves exactly like a ReplicationController, but it has <b>more expressive pod selectors.</b>
</p>
</div>

<div id="outline-container-orga24f338" class="outline-4">
<h4 id="orga24f338"><span class="section-number-4">1.4.1</span> More Expressive Label Selectors</h4>
<div class="outline-text-4" id="text-1-4-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchExpressions:
      - key: app                # this selector requires the pod to contain a label with the 'app' key
        operator: In
        values:
         - kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
</pre>
</div>

<p>
Each expression must contain <span class="underline">a key</span>, <span class="underline">an operator</span>, and possibly (depending on the operator) <span class="underline">a list of values</span>. <br>
Four valid operators:
</p>

<dl class="org-dl">
<dt>In</dt><dd>Label’s value must match one of the specified <span class="underline">values</span>.</dd>
<dt>NotIn</dt><dd>Label’s value must not match any of the specified <span class="underline">values</span>.</dd>
<dt>Exists</dt><dd>Pod must include a label with the specified key (the value isn’t important). When using this operator, you shouldn’t specify the <span class="underline">values</span> field.</dd>
<dt>DoesNotExist</dt><dd>Pod must not include a label with the specified key. The <span class="underline">values</span> property must not be specified.</dd>
</dl>
</div>
</div>


<div id="outline-container-orgfd58380" class="outline-4">
<h4 id="orgfd58380"><span class="section-number-4">1.4.2</span> Only cares about number of pods</h4>
<div class="outline-text-4" id="text-1-4-2">

<div class="figure">
<p><img src="img/k8s_rs_only_care_num.png" alt="k8s_rs_only_care_num.png">
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org84831a0" class="outline-3">
<h3 id="org84831a0"><span class="section-number-3">1.5</span> DaemonSet</h3>
<div class="outline-text-3" id="text-1-5">
<p>
DaemonSet is used when you want <b>a pod to run on each and every node in the cluster</b> <br>
(and each node needs to run exactly <b>one instance of the pod</b>).
</p>


<div class="figure">
<p><img src="img/k8s_ds.png" alt="k8s_ds.png">
</p>
</div>

<p>
To run a pod on all cluster nodes, you create a DaemonSet object, which is much like a ReplicationController or a ReplicaSet, <br>
except that pods created by a DaemonSet already have a target node specified and <b>skip the Kubernetes Scheduler.</b>
</p>
</div>
</div>


<div id="outline-container-org3ecdedc" class="outline-3">
<h3 id="org3ecdedc"><span class="section-number-3">1.6</span> Job</h3>
<div class="outline-text-3" id="text-1-6">
<p>
<b>In the event of a node failure</b>, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are. <br>
<b>In the event of a failure of the process itself</b> (when the process returns an error exit code), <br>
the Job can be configured to either restart the container or not.
</p>
</div>


<div id="outline-container-org72396e7" class="outline-4">
<h4 id="org72396e7"><span class="section-number-4">1.6.1</span> Definition</h4>
<div class="outline-text-4" id="text-1-6-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure  # Jobs can't use the default restart policy, which is Always
      containers:
      - name: main
        image: luksa/batch-job
</pre>
</div>


<p>
Job pods can't use the default policy, because they're not meant to run indefinitely. <br>
Need to explicitly set the restart policy to either <code>OnFailure</code> or <code>Never</code>.
</p>
</div>
</div>


<div id="outline-container-org16dd420" class="outline-4">
<h4 id="org16dd420"><span class="section-number-4">1.6.2</span> Limit time for Job pod to complete</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
A pod's time can be limited by setting the <code>activeDeadlineSeconds</code> property in the pod spec. <br>
If the pod runs longer than that, the system will try to terminate it and will mark the Job as failed.
</p>

<pre class="example">
You can configure how many times a Job can be retried before it is marked as failed by specifying the spec.backoffLimit field in the Job manifest.
If you don't explicitly specify it, it defaults to 6.
</pre>
</div>
</div>
</div>



<div id="outline-container-org948c35b" class="outline-3">
<h3 id="org948c35b"><span class="section-number-3">1.7</span> CronJob</h3>
<div class="outline-text-3" id="text-1-7">
<p>
A CronJob creates Job resources from the <code>jobTemplate</code> property configured in the CronJob spec. <br>
The Job then creates the pods.
</p>
</div>
</div>


<div id="outline-container-org1d579a5" class="outline-3">
<h3 id="org1d579a5"><span class="section-number-3">1.8</span> Service</h3>
<div class="outline-text-3" id="text-1-8">
</div>
<div id="outline-container-org4cfaaa9" class="outline-4">
<h4 id="org4cfaaa9"><span class="section-number-4">1.8.1</span> Overall</h4>
<div class="outline-text-4" id="text-1-8-1">
<p>
Although the primary purpose of services is exposing groups of pods to other pods <b>in the cluster</b>, <br>
<b>Both</b> internal (by DNS or ENV) and external (by IP) clients usually connect to pods through services:
</p>


<div class="figure">
<p><img src="img/k8s_svc_overall.png" alt="k8s_svc_overall.png">
</p>
</div>

<p>
<b>Label selectors</b> determine which pods belong to the Service:
</p>


<div class="figure">
<p><img src="img/k8s_svc_label.png" alt="k8s_svc_label.png">
</p>
</div>
</div>
</div>


<div id="outline-container-orgfb7e821" class="outline-4">
<h4 id="orgfb7e821"><span class="section-number-4">1.8.2</span> Creation</h4>
<div class="outline-text-4" id="text-1-8-2">
</div>
<div id="outline-container-org8081c57" class="outline-5">
<h5 id="org8081c57"><span class="section-number-5">1.8.2.1</span> By <code>kubectl expose</code></h5>
</div>


<div id="outline-container-orgcd7df6e" class="outline-5">
<h5 id="orgcd7df6e"><span class="section-number-5">1.8.2.2</span> By YAML</h5>
<div class="outline-text-5" id="text-1-8-2-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80                    # the port this service will be available on
    targetPort: 8080            # the container port the service will forward to
  selector:
    app: kubia                  # all pods with the app=kubia label will be part of this service
</pre>
</div>
</div>

<div id="outline-container-org20dd7c7" class="outline-6">
<h6 id="org20dd7c7"><span class="section-number-6">1.8.2.2.1</span> Expose ports in same Service</h6>
<div class="outline-text-6" id="text-1-8-2-2-1">
<pre class="example">
When creating a service with multiple ports, you must specify a name for each port.
</pre>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec: ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
</pre>
</div>
</div>
</div>


<div id="outline-container-orge594d4b" class="outline-6">
<h6 id="orge594d4b"><span class="section-number-6">1.8.2.2.2</span> Using Named Ports</h6>
<div class="outline-text-6" id="text-1-8-2-2-2">

<div class="figure">
<p><img src="img/k8s_svc_named_ports_container.png" alt="k8s_svc_named_ports_container.png">
</p>
</div>

<p>
You can then refer to those ports by name in the service spec:
</p>


<div class="figure">
<p><img src="img/k8s_svc_named_ports_pod.png" alt="k8s_svc_named_ports_pod.png">
</p>
</div>

<pre class="example">
The biggest benefit of doing so is that it enables you to change port numbers later without having to change the service spec.
</pre>
</div>
</div>
</div>
</div>





<div id="outline-container-org6fede82" class="outline-4">
<h4 id="org6fede82"><span class="section-number-4">1.8.3</span> Session Affinity</h4>
<div class="outline-text-4" id="text-1-8-3">
<p>
If you want all requests made by a certain client to be redirected to the <b>same</b> pod every time, <br>
you can set the service's <span class="underline">sessionAffinity</span> property to <code>ClientIP</code> (instead of <code>None</code>, which is the default).
</p>

<p>
This makes the service proxy redirect all requests originating <b>from the same client IP to the same pod.</b>
</p>


<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
...
</pre>
</div>
</div>
</div>





<div id="outline-container-org769331e" class="outline-4">
<h4 id="org769331e"><span class="section-number-4">1.8.4</span> Discovering Service</h4>
<div class="outline-text-4" id="text-1-8-4">
</div>
<div id="outline-container-org5de9cad" class="outline-5">
<h5 id="org5de9cad"><span class="section-number-5">1.8.4.1</span> Through ENV</h5>
<div class="outline-text-5" id="text-1-8-4-1">
<pre class="example">
Dashes in the service name are converted to underscores and all letters are uppercased
when the service name is used as the prefix in the environment variable's name.
</pre>
</div>
</div>

<div id="outline-container-org3152ab8" class="outline-5">
<h5 id="org3152ab8"><span class="section-number-5">1.8.4.2</span> Through DNS</h5>
<div class="outline-text-5" id="text-1-8-4-2">
<pre class="example">
kubia        .default  .svc.cluster.local
------------
service name
             ---------
             namespace
                       ------------------
                       configurable cluster domain suffix
</pre>

<p>
<b>You can omit the <code>svc.cluster.local</code> suffix and even the namespace, when pods are in the same namespace.</b> <br>
(This is because how <code>/etc/resolv.conf</code> is configured)
</p>


<pre class="example">
Whether a pod uses the internal DNS server or not is configurable through the 'dnsPolicy' property in each pod's spec.
</pre>
</div>
</div>
</div>


<div id="outline-container-org7a30639" class="outline-4">
<h4 id="org7a30639"><span class="section-number-4">1.8.5</span> Endpoints</h4>
<div class="outline-text-4" id="text-1-8-5">
<p>
Services don't link to pods directly, but <b>Endpoints</b>. <br>
An Endpoints resource (plural) is <b>a list of IP addresses and ports</b> exposing a service.
</p>


<div class="figure">
<p><img src="img/k8s_svc_endpoints.png" alt="k8s_svc_endpoints.png">
</p>
</div>


<pre class="example">
Although the pod selector is defined in the service spec, it's not used directly when redirecting incoming connections.
Instead, the selector is used to build a list of IPs and ports, which is then stored in the Endpoints resource.

If you create a service without a pod selector, Kubernetes won't even create the Endpoints resource
(after all, without a selector, it can't know which pods to include in the service).
It's up to you to create the Endpoints resource to specify the list of endpoints for the service.
</pre>
</div>

<div id="outline-container-org7b803eb" class="outline-5">
<h5 id="org7b803eb"><span class="section-number-5">1.8.5.1</span> Manually config service endpoints</h5>
<div class="outline-text-5" id="text-1-8-5-1">
<p>
To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource.
</p>

<p>
Define a service called external-service that will accept incoming connections on port 80 (didn't define a pod selector for the service):
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: external-service        # the name of the service must match the name of the Endpoints object
spec:                           # this service has no selector defined
  ports:
  - port: 80
</pre>
</div>


<p>
The Endpoints object needs to <b>have the same name as the service</b> and contain the list of target IP addresses and ports for the service:
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Endpoints
metadata:
  name: external-service        # the name of the Endpoints object must match the name of the service
subsets:
  - addresses:
    - ip: 11.11.11.11           # | IPs of the endpoints that the service will forward connections to
    - ip: 22.22.22.22           # |
    ports:
    - port: 80                  # target port of the endpoints
</pre>
</div>

<p>
After both the Service and the Endpoints resource are posted to the server, <br>
the service is ready to be used like any regular service with a pod selector:
</p>


<div class="figure">
<p><img src="img/k8s_svc_manual_ep_external.png" alt="k8s_svc_manual_ep_external.png">
</p>
</div>

<p>
If you later decide to migrate the external service to pods running inside Kubernetes, <br>
you can add a selector to the service, thereby making its Endpoints managed automatically.
The same is also true in reverse by removing the selector from a Service, Kubernetes stops updating its Endpoints. <br>
This means a service IP address can remain constant while the actual implementation of the service is changed.
</p>
</div>
</div>





<div id="outline-container-org44796f7" class="outline-5">
<h5 id="org44796f7"><span class="section-number-5">1.8.5.2</span> Creat alias for external service</h5>
<div class="outline-text-5" id="text-1-8-5-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: api.somecompany.com # fully qualified domain name of the actual service
  ports:
  - port: 80
</pre>
</div>

<p>
After the service is created, pods can connect to the external service through the <code>external-service.default.svc.cluster.local</code> domain name.
</p>

<pre class="example">
ExternalName services are implemented solely at the DNS level: a simple CNAME DNS record is created for the service.
Therefore, clients connecting to the service will connect to the external service directly, bypassing the service proxy completely.
For this reason, these types of services don't even get a cluster IP.
</pre>
</div>
</div>
</div>



<div id="outline-container-orgf6724e7" class="outline-4">
<h4 id="orgf6724e7"><span class="section-number-4">1.8.6</span> Exposing Services To External Clients</h4>
<div class="outline-text-4" id="text-1-8-6">
<p>
A few ways to make a service accessible externally:
</p>

<ol class="org-ol">
<li>Setting the service type to NodePort</li>
<li>Setting the service type to LoadBalancer, an extension of the NodePort type</li>
<li>Creating an Ingress resource</li>
</ol>
</div>

<div id="outline-container-orgdf046ad" class="outline-5">
<h5 id="orgdf046ad"><span class="section-number-5">1.8.6.1</span> NodePort</h5>
<div class="outline-text-5" id="text-1-8-6-1">

<div class="figure">
<p><img src="img/k8s_svc_node_port_overall.png" alt="k8s_svc_node_port_overall.png">
</p>
</div>

<p>
An incoming connection to one of those ports will be redirected to a randomly selected pod, <br>
which may or may not be the one running on the node the connection is being made to.
</p>

<pre class="example">
If you only point your clients to the first node, when that node fails, your clients can't access the service anymore.
That's why it makes sense to put a load balancer in front of the nodes to make sure you're spreading requests across all healthy nodes and never sending them to a node that's offline at that moment.
</pre>
</div>

<div id="outline-container-orgdd5e055" class="outline-6">
<h6 id="orgdd5e055"><span class="section-number-6">1.8.6.1.1</span> YAML Definition</h6>
<div class="outline-text-6" id="text-1-8-6-1-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80                    # port of the service's internal cluster IP
    targetPort: 8080            # target port of the backing pods
    nodePort: 30123             # service will be accessible through port 30123 of each of cluster nodes
  selector:
    app: kubia
</pre>
</div>

<pre class="example">
Specifying the port isn't mandatory; Kubernetes will choose a random port if you omit it.
</pre>
</div>
</div>
</div>



<div id="outline-container-orgaf84383" class="outline-5">
<h5 id="orgaf84383"><span class="section-number-5">1.8.6.2</span> LoadBalancer</h5>
<div class="outline-text-5" id="text-1-8-6-2">
<p>
Kubernetes clusters running on cloud providers usually support the automatic provision of a load balancer from the cloud infrastructure. <br>
The load balancer will have its own unique, publicly accessible IP address and will redirect all connections to your service. <br>
You can thus access your service through the load balancer's IP address.
</p>

<p>
If Kubernetes is running in an environment that doesn't support LoadBalancer services, <br>
the load balancer will not be provisioned, but the service will still behave like a NodePort service. <br>
That's because <b>a LoadBalancer service is an extension of a NodePort service.</b> <br>
(LoadBalancer type service is still a NodePort service but with an additional infrastructure-provided load balancer)
</p>



<div class="figure">
<p><img src="img/k8s_svc_lb_overall.png" alt="k8s_svc_lb_overall.png">
</p>
</div>
</div>


<div id="outline-container-org699e3cb" class="outline-6">
<h6 id="org699e3cb"><span class="section-number-6">1.8.6.2.1</span> YAML Definition</h6>
<div class="outline-text-6" id="text-1-8-6-2-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer            # this type of service obtains a load balancer from the infrastructur hosting the Kubernetes cluster
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-orga3715a1" class="outline-5">
<h5 id="orga3715a1"><span class="section-number-5">1.8.6.3</span> Ingress</h5>
<div class="outline-text-5" id="text-1-8-6-3">
<p>
One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, <br>
whereas an Ingress <b>only requires one</b>, even when providing access to dozens of services.
</p>

<p>
Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, <br>
which services can’t.
</p>



<div class="figure">
<p><img src="img/k8s_svc_ingress_overall.png" alt="k8s_svc_ingress_overall.png">
</p>
</div>

<pre class="example">
Ingress controller provisions a load balancer behind the scenes.
</pre>
</div>




<div id="outline-container-org3816ce4" class="outline-6">
<h6 id="org3816ce4"><span class="section-number-6">1.8.6.3.1</span> YAML Definition</h6>
<div class="outline-text-6" id="text-1-8-6-3-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com     # maps 'kubia.example.com' domain name to your service
    http:
      paths:
      - path: /                       # | all requests will be sent to port 80 of the kubia-nodeport service
        backend:                      # |
          serviceName: kubia-nodeport # |
          servicePort: 80             # |
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_svc_ingress_process.png" alt="k8s_svc_ingress_process.png">
</p>
</div>


<p>
Ingress controller didn’t forward the request to the service. <b>It only used it to select a pod</b>. (Most controllers work like this) <br>
This affects the preservation of client IPs when external clients connect through the Ingress controller,
which makes them preferred over Services in certain use cases.
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgdc5ecb8" class="outline-4">
<h4 id="orgdc5ecb8"><span class="section-number-4">1.8.7</span> Readiness Probe</h4>
<div class="outline-text-4" id="text-1-8-7">
<p>
Kubernetes invokes the probe periodically and acts based on the result of the readiness probe. <br>
If a pod reports that it’s not ready, it’s removed from the service. If the pod then becomes ready again, it’s re-added.
</p>

<p>
Unlike liveness probes, if a container fails the readiness check, it won’t be killed or restarted. <br>
The effect is the same as when the pod doesn’t match the service’s label selector at all:
</p>


<div class="figure">
<p><img src="img/k8s_svc_readiness_probe.png" alt="k8s_svc_readiness_probe.png">
</p>
</div>
</div>
</div>


<div id="outline-container-orgd8de87e" class="outline-4">
<h4 id="orgd8de87e"><span class="section-number-4">1.8.8</span> Headless</h4>
<div class="outline-text-4" id="text-1-8-8">
<p>
Kubernetes allows clients to <b>discover pod IPs</b> through DNS lookups. <br>
Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP (the service’s cluster IP). <br>
By setting the <code>clusterIP</code> field to <code>None</code> in the service specification, the DNS server will return the pod IPs instead of the single service IP.
</p>

<p>
Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, <br>
each pointing to the IP of an individual pod backing the service at that moment.
</p>

<pre class="example">
A headless services still provides load balancing across pods,
but through the DNS round-robin mechanism instead of through the service proxy.
</pre>
</div>
</div>

<div id="outline-container-orgdff01e4" class="outline-4">
<h4 id="orgdff01e4"><span class="section-number-4">1.8.9</span> External Traffic Issue</h4>
<div class="outline-text-4" id="text-1-8-9">
<p>
When an external client connects to a service through the node port (this also includes cases when it goes through the load balancer first), <br>
the randomly chosen pod may or may not be running on the same node that received the connection. <br>
An additional network hop is required to reach the pod, but this may not always be desirable.
</p>

<p>
You can prevent this additional hop by configuring the service to redirect external traffic only to pods running on the node that received the connection. <br>
This is done by setting the <code>externalTrafficPolicy</code> field in the service's spec section:
</p>

<div class="org-src-container">
<pre class="src src-yaml">spec:
  externalTrafficPolicy: Local
  ...
</pre>
</div>

<p>
f a service definition includes this setting and an external connection is opened through the service's node port, <br>
the service proxy will choose a locally running pod. If no local pods exist, the connection will <b>hang</b>. <br>
You therefore need to ensure the load balancer forwards connections only to nodes that <b>have at least one such pod</b>.
</p>
</div>

<div id="outline-container-org09c0e7c" class="outline-5">
<h5 id="org09c0e7c"><span class="section-number-5">1.8.9.1</span> Non-Preservation Of The Client's IP Issue</h5>
<div class="outline-text-5" id="text-1-8-9-1">
<p>
Usually, when clients inside the cluster connect to a service, the pods backing the service <b>can obtain the client's IP address</b>. <br>
But when the connection is received through a node port, the packets' <b>source IP is changed</b>, because SNAT is performed on the packets.
</p>

<p>
The <span class="underline">Local</span> external traffic policy (externalTrafficPolicy: Local) described in the previous section affects the preservation of the client's IP, <br>
because there's no additional hop between the node receiving the connection and the node hosting the target pod (<b>SNAT isn't performed</b>).
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgc8fbabd" class="outline-3">
<h3 id="orgc8fbabd"><span class="section-number-3">1.9</span> Volume</h3>
<div class="outline-text-3" id="text-1-9">
<p>
Volumes aren't top-level resources like pods, but are instead defined as <b>a part of a pod</b> and <b>share the same lifecycle as the pod.</b> <br>
This means <span class="underline">a volume is created when the pod is started and is destroyed when the pod is deleted.</span>
</p>

<pre class="example">
Kubernetes volumes are a component of a pod and are thus defined in the pod's specification-much like containers.
They aren't a standalone Kubernetes object and can not be created or deleted on their own.
</pre>


<div class="figure">
<p><img src="img/k8s_vol.png" alt="k8s_vol.png">
</p>
</div>
</div>

<div id="outline-container-org94a09be" class="outline-4">
<h4 id="org94a09be"><span class="section-number-4">1.9.1</span> Volume Types</h4>
<div class="outline-text-4" id="text-1-9-1">
<dl class="org-dl">
<dt>emptyDir</dt><dd>A simple empty directory used for storing transient data.</dd>
<dt>hostPath</dt><dd>Used for mounting directories from the worker node's filesystem into the pod.</dd>
<dt>gitRepo</dt><dd>A volume initialized by checking out the contents of a Git repository.</dd>
<dt>nfs</dt><dd>An NFS share mounted into the pod.</dd>
<dt>gcePersistentDisk, awsElasticBlockStore, azureDisk</dt><dd>Cloud provider.</dd>
<dt>cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere- Volume, photonPersistentDisk, scaleIO</dt><dd>Network storage.</dd>
<dt>configMap, secret, downwardAPI</dt><dd></dd>

<dt>persistentVolumeClaim</dt><dd></dd>
</dl>
</div>
</div>


<div id="outline-container-org03ab9c1" class="outline-4">
<h4 id="org03ab9c1"><span class="section-number-4">1.9.2</span> emptyDir</h4>
<div class="outline-text-4" id="text-1-9-2">
<p>
An <code>emptyDir</code> volume is especially useful for sharing files between containers running in the same pod.
</p>

<p>
An <code>emptyDir</code> volume is the simplest type of volume, but <b>other types build upon it</b>. (After the empty directory is created, they then populate it with data)
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html # the same volume as above is mounted at /usr/share/nginx/html as read-only
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html                  # mounted in the two containers above
    emptyDir: {}
</pre>
</div>
</div>





<div id="outline-container-org82d0528" class="outline-5">
<h5 id="org82d0528"><span class="section-number-5">1.9.2.1</span> The medium used</h5>
<div class="outline-text-5" id="text-1-9-2-1">
<p>
The <code>emptyDir</code> used as the volume was created on the actual disk of the worker node hosting the pod. <br>
Kubernetes can create the <code>emptyDir</code> on a tmpfs filesystem (in memory instead of on disk). <br>
To do this, set the <code>emptyDir</code>'s medium to <code>Memory</code>:
</p>

<div class="org-src-container">
<pre class="src src-yaml">volumes:
  - name: html
    emptyDir:
      medium: Memory
</pre>
</div>
</div>
</div>
</div>












<div id="outline-container-org7a6750b" class="outline-4">
<h4 id="org7a6750b"><span class="section-number-4">1.9.3</span> hostPath</h4>
<div class="outline-text-4" id="text-1-9-3">

<div class="figure">
<p><img src="img/k8s_vol_host.png" alt="k8s_vol_host.png">
</p>
</div>

<p>
It’s not a good idea to use a hostPath volume for regular pods, because it makes the pod sensitive to what node it’s scheduled to.
</p>

<pre class="example">
Remember to use hostPath volumes only if you need to read or write system files on the node. Never use them to persist data across pods.
</pre>
</div>
</div>



<div id="outline-container-org09da27c" class="outline-4">
<h4 id="org09da27c"><span class="section-number-4">1.9.4</span> PV/PVC</h4>
<div class="outline-text-4" id="text-1-9-4">

<div class="figure">
<p><img src="img/k8s_vol_pv_pvc.png" alt="k8s_vol_pv_pvc.png">
</p>
</div>


<p>
Instead of the developer adding a technology-specific volume to their pod, it’s the <b>cluster administrator</b> who sets up the underlying storage and then <br>
registers it in Kubernetes by creating a PersistentVolume resource through the Kubernetes API server.
</p>

<p>
When a cluster user needs to use persistent storage in one of their pods, they first create a PersistentVolumeClaim manifest,
specifying the minimum size and the access mode they require. <br>
The user then submits the PersistentVolumeClaim manifest to the Kubernetes API server,
and Kubernetes finds the appropriate PersistentVolume and <b>binds the volume to the claim.</b> <br>
The PersistentVolumeClaim can then be used as one of the volumes inside a pod.
</p>

<pre class="example">
Other users cannot use the same PersistentVolume until it has been released by deleting the bound PersistentVolumeClaim.
</pre>
</div>

<div id="outline-container-org4663291" class="outline-5">
<h5 id="org4663291"><span class="section-number-5">1.9.4.1</span> Creating PersistentVolume</h5>
<div class="outline-text-5" id="text-1-9-4-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:
    storage: 1Gi                # Defining the PersistentVolume's size
  accessModes:                  # Cwhether it can be read from and/or written to by a single node or by multiple nodes at the same time.
    - ReadWriteOnce
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain # After the claim is released, the PersistentVolume should be retained (not erased or deleted).
                                        # Have to manually delete pv when it is no more needed. (no deleted when pvc is deleted)
  hostPath:
    path: /tmp/mongodb
</pre>
</div>

<pre class="example">
PersistentVolumes don’t belong to any namespace. They’re cluster-level resources like nodes.
</pre>

<p>
(<b>注:</b> <code>spec.capacity.storage</code> 看起来只是用于匹配 PVC ，实际容量应以 Actual storage 为准)
</p>


<div class="figure">
<p><img src="img/k8s_vol_pv_pvc_overall.png" alt="k8s_vol_pv_pvc_overall.png">
</p>
</div>
</div>


<div id="outline-container-orgc5191bd" class="outline-6">
<h6 id="orgc5191bd"><span class="section-number-6">1.9.4.1.1</span> Access Mode</h6>
<div class="outline-text-6" id="text-1-9-4-1-1">
<dl class="org-dl">
<dt>ReadWriteOnce</dt><dd>Only a single node can mount the volume for reading and writing.</dd>
<dt>ReadOnlyMany</dt><dd>Multiple nodes can mount the volume for reading.</dd>
<dt>ReadWriteMany</dt><dd>Multiple nodes can mount the volume for both reading and writing.</dd>
</dl>

<pre class="example">
RWO, ROX, and RWX pertain to the number of worker nodes that can use the volume at the same time, not to the number of pods!
</pre>
</div>
</div>
</div>




<div id="outline-container-orge8e5aa9" class="outline-5">
<h5 id="orge8e5aa9"><span class="section-number-5">1.9.4.2</span> Creating PersistentVolumeClaim</h5>
<div class="outline-text-5" id="text-1-9-4-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
  - ReadWriteOnce
  storageClassName: ""  # Specifying an empty string as the storage class name ensures the PVC binds to a pre-provisioned PV instead of dynamically provisioning a new one.
</pre>
</div>


<p>
As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume and binds it to the claim. <br>
The PersistentVolume’s capacity must be <b>large enough</b> to accommodate what the claim requests.
Additionally, the volume’s access modes must <b>include</b> the access modes requested by the claim.
</p>
</div>
</div>


<div id="outline-container-org09e87af" class="outline-5">
<h5 id="org09e87af"><span class="section-number-5">1.9.4.3</span> Using PersistentVolumeClaim in Pod</h5>
<div class="outline-text-5" id="text-1-9-4-3">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc    # referenced by name

</pre>
</div>



<div class="figure">
<p><img src="img/k8s_vol_pv_pvc_logic.png" alt="k8s_vol_pv_pvc_logic.png">
</p>
</div>
</div>
</div>


<div id="outline-container-orgc106a31" class="outline-5">
<h5 id="orgc106a31"><span class="section-number-5">1.9.4.4</span> Dynamic Provisioning of PV</h5>
<div class="outline-text-5" id="text-1-9-4-4">
<p>
Cluster administrator can define <code>StorageClass</code> and let the system create a new PersistentVolume <b>each time</b> one is requested through a PersistentVolumeClaim.
</p>

<pre class="example">
Similar to PersistentVolumes, StorageClass resources aren’t namespaced.
</pre>


<div class="figure">
<p><img src="img/k8s_vol_pv_dp.png" alt="k8s_vol_pv_dp.png">
</p>
</div>
</div>

<div id="outline-container-org4aaaca8" class="outline-6">
<h6 id="org4aaaca8"><span class="section-number-6">1.9.4.4.1</span> Defining StorageClass</h6>
<div class="outline-text-6" id="text-1-9-4-4-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: k8s.io/minikube-hostpath # The volume plugin to use for provisioning the PersistentVolume
parameters:                           # Parameterd passed to the provisioner
  type: pd-ssd
</pre>
</div>
</div>
</div>


<div id="outline-container-org870382f" class="outline-6">
<h6 id="org870382f"><span class="section-number-6">1.9.4.4.2</span> Requesting the SC in a PVC</h6>
<div class="outline-text-6" id="text-1-9-4-4-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
</pre>
</div>

<pre class="example">
The provisioner is used even if an existing manually provisioned PersistentVolume matches the PersistentVolumeClaim.
</pre>
</div>
</div>


<div id="outline-container-org957ab41" class="outline-6">
<h6 id="org957ab41"><span class="section-number-6">1.9.4.4.3</span> Creating a PVC Without SC</h6>
<div class="outline-text-6" id="text-1-9-4-4-3">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2
spec:
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
</pre>
</div>

<pre class="example">
The default storage class is what's used to dynamically provision a PersistentVolume if the PersistentVolumeClaim doesn't explicitly say which storage class to use.
</pre>
</div>
</div>




<div id="outline-container-orgddb26a7" class="outline-6">
<h6 id="orgddb26a7"><span class="section-number-6">1.9.4.4.4</span> Forcing a PVC to Be Bound to One of The Pre-Provisioned PVs</h6>
<div class="outline-text-6" id="text-1-9-4-4-4">
<pre class="example">
Explicitly set storageClassName to "" if you want the PVC to use a pre-provisioned PersistentVolume.
</pre>
</div>
</div>
</div>
</div>
</div>






<div id="outline-container-org2a59b4e" class="outline-3">
<h3 id="org2a59b4e"><span class="section-number-3">1.10</span> ConfigMap &amp; Secret</h3>
<div class="outline-text-3" id="text-1-10">
</div>
<div id="outline-container-orgb7c84f0" class="outline-4">
<h4 id="orgb7c84f0"><span class="section-number-4">1.10.1</span> Basic</h4>
<div class="outline-text-4" id="text-1-10-1">
</div>
<div id="outline-container-org009130c" class="outline-5">
<h5 id="org009130c"><span class="section-number-5">1.10.1.1</span> Overriding the command and arguments</h5>
<div class="outline-text-5" id="text-1-10-1-1">
<p>
In Kubernetes, when specifying a container, you can choose to override both <code>ENTRYPOINT</code> and <code>CMD</code>.
</p>


<div class="figure">
<p><img src="img/k8s_config_overriding.png" alt="k8s_config_overriding.png">
</p>
</div>


<p>
In most cases, you’ll only set custom arguments and rarely override the command
(except in <b>general-purpose images such as busybox</b>, which doesn’t define an <code>ENTRYPOINT</code> at all).
</p>

<pre class="example">
The command and args fields can’t be updated after the pod is created.
</pre>
</div>
</div>


<div id="outline-container-org7838b3d" class="outline-5">
<h5 id="org7838b3d"><span class="section-number-5">1.10.1.2</span> Specifing env variables</h5>
<div class="outline-text-5" id="text-1-10-1-2">
<div class="org-src-container">
<pre class="src src-yaml">kind: Pod
spec:
 containers:
 - image: luksa/fortune:env
   env:                         # Adding a single variable to the environment variable list
   - name: INTERVAL
     value: "30"
   name: html-generator
</pre>
</div>

<div class="org-src-container">
<pre class="src src-yaml">env:
- name: FIRST_VAR
  value: "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"      # Referring to other environment variables
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org34d6d8f" class="outline-4">
<h4 id="org34d6d8f"><span class="section-number-4">1.10.2</span> ConfigMap</h4>
<div class="outline-text-4" id="text-1-10-2">

<div class="figure">
<p><img src="img/k8s_config_map.png" alt="k8s_config_map.png">
</p>
</div>
</div>


<div id="outline-container-orgaba7703" class="outline-5">
<h5 id="orgaba7703"><span class="section-number-5">1.10.2.1</span> Creating</h5>
<div class="outline-text-5" id="text-1-10-2-1">
</div>
<div id="outline-container-org0e731a7" class="outline-6">
<h6 id="org0e731a7"><span class="section-number-6">1.10.2.1.1</span> CLI</h6>
<div class="outline-text-6" id="text-1-10-2-1-1">
<div class="org-src-container">
<pre class="src src-sh">kubectl create configmap my-config <span style="color: #e6db74;">\</span>
        --from-file=foo.json <span style="color: #e6db74;">\ </span> <span style="color: #465457;"># </span><span style="color: #465457;">A single file</span>
        --from-file=<span style="color: #fd971f;">bar</span>=foobar.conf <span style="color: #e6db74;">\ </span><span style="color: #465457;"># </span><span style="color: #465457;">A file stored under a custom key</span>
        --from-file=config-opts/ <span style="color: #e6db74;">\ </span>A whole directory
        --from-literal=<span style="color: #fd971f;">some</span>=thing <span style="color: #465457;"># </span><span style="color: #465457;">A literal value</span>
</pre>
</div>



<div class="figure">
<p><img src="img/k8s_config_map_cli.png" alt="k8s_config_map_cli.png">
</p>
</div>
</div>
</div>



<div id="outline-container-orgbdb9b8b" class="outline-6">
<h6 id="orgbdb9b8b"><span class="section-number-6">1.10.2.1.2</span> YAML</h6>
<div class="outline-text-6" id="text-1-10-2-1-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: fortune-config
data:
  sleep-interval: "25"
</pre>
</div>
</div>
</div>
</div>




<div id="outline-container-org80e78e2" class="outline-5">
<h5 id="org80e78e2"><span class="section-number-5">1.10.2.2</span> Passing CM to Container</h5>
<div class="outline-text-5" id="text-1-10-2-2">
</div>
<div id="outline-container-orga25705c" class="outline-6">
<h6 id="orga25705c"><span class="section-number-6">1.10.2.2.1</span> As ENV</h6>
<div class="outline-text-6" id="text-1-10-2-2-1">

<div class="figure">
<p><img src="img/k8s_config_map_env.png" alt="k8s_config_map_env.png">
</p>
</div>

<ul class="org-ul">
<li><p>
Passing all entries of a ConfigMap as environment variables at once
</p>

<div class="org-src-container">
<pre class="src src-yaml">spec:
  containers:
  - image: some-image
    envFrom:                    # Using envFrom instead of env
    - prefix: CONFIG_           # All environment variables will be prefixed with CONFIG_.
      configMapRef:
        name: my-config-map     # Referencing the ConfigMap called my-config-map

</pre>
</div></li>
</ul>

<pre class="example">
The prefix is optional, so if you omit it the environment variables will have the same name as the keys.
</pre>

<ul class="org-ul">
<li><p>
Passing a ConfigMap entry as a command-line argument
</p>

<p>
You can’t reference ConfigMap entries directly in the <code>pod.spec.containers.args</code> field,
but you can first initialize an environment variable from the ConfigMap entry and then refer to the variable inside the arguments:
</p>


<div class="figure">
<p><img src="img/k8s_config_map_env_args.png" alt="k8s_config_map_env_args.png">
</p>
</div></li>
</ul>
</div>
</div>




<div id="outline-container-orgac017d9" class="outline-6">
<h6 id="orgac017d9"><span class="section-number-6">1.10.2.2.2</span> As Volume</h6>
<div class="outline-text-6" id="text-1-10-2-2-2">
<p>
A <code>configMap</code> volume will expose each entry of the ConfigMap <b>as a file.</b>
The process running in the container can obtain the entry’s value by reading the contents of the file.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d #  mount configMap volume at this location
      readOnly: true
    - name: config
      mountPath: /tmp/whole-fortune-config-volume
      readOnly: true
    ports:
      - containerPort: 80
        name: http
        protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:                  # the volume refers to fortune-config ConfigMap
      name: fortune-config
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_config_map_vol.png" alt="k8s_config_map_vol.png">
</p>
</div>


<ul class="org-ul">
<li><p>
Exposing Certain ConfigMap Entries in the Volume
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume-with-items
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d/
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:                    # select which entries to include in the volume by listing them
      - key: my-nginx-config.conf # entry under this key included
        path: gzip.conf           # entry's value should be stored in this file

</pre>
</div></li>
</ul>




<ul class="org-ul">
<li><p>
Mounting Individual ConfigMap Entries as Files without Hiding Other Files in the Directory
</p>


<div class="figure">
<p><img src="img/k8s_config_map_vol_with_specific_items.png" alt="k8s_config_map_vol_with_specific_items.png">
</p>
</div>


<div class="figure">
<p><img src="img/k8s_config_map_vol_with_specific_items_pic.png" alt="k8s_config_map_vol_with_specific_items_pic.png">
</p>
</div></li>
</ul>


<pre class="example">
The 'subPath' property can be used when mounting any kind of volume. Instead of mounting the whole volume, you can mount part of it.
</pre>




<ul class="org-ul">
<li><p>
Setting the File Permissions for Files in a ConfigMap Volume
</p>

<div class="org-src-container">
<pre class="src src-yaml">volumes:
- name: config
  configMap:
    name: fortune-config
    defaultMode: "6600"         # This sets the permissions for all files to -rw-rw------
</pre>
</div></li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-orga8fe4c7" class="outline-4">
<h4 id="orga8fe4c7"><span class="section-number-4">1.10.3</span> Secret</h4>
<div class="outline-text-4" id="text-1-10-3">
<p>
Kubernetes helps keep your Secrets safe by making sure each Secret <b>is only distributed to the nodes</b> that run the pods that need access to the Secret. <br>
Also, on the nodes themselves, Secrets are always <b>stored in memory and never written to physical storage.</b>
</p>

<pre class="example">
The maximum size of a Secret is limited to 1MB.
</pre>
</div>


<div id="outline-container-orge80196a" class="outline-5">
<h5 id="orge80196a"><span class="section-number-5">1.10.3.1</span> Default Token Secret</h5>
<div class="outline-text-5" id="text-1-10-3-1">
<p>
<b>Every pod has a secret volume</b> attached to it automatically, which represent everything you need to securely talk to the Kubernetes API server from within your pods, should you need to do that.
</p>

<pre class="example">
By default, the default-token Secret is mounted into every container, but you can disable that in each pod
by setting the automountServiceAccountToken field in the pod spec to false or by setting it to false on the service account the pod is using.
</pre>



<div class="figure">
<p><img src="img/k8s_secret_default_token.png" alt="k8s_secret_default_token.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org4f8b3b6" class="outline-5">
<h5 id="org4f8b3b6"><span class="section-number-5">1.10.3.2</span> Image Pull Secret</h5>
<div class="outline-text-5" id="text-1-10-3-2">
<div class="org-src-container">
<pre class="src src-sh">kubectl create secret docker-registry mydockerhubsecret <span style="color: #e6db74;">\</span>
  --docker-username=myusername --docker-password=mypassword <span style="color: #e6db74;">\</span>
  --docker-email=my.email@provider.com

</pre>
</div>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: private-pod
spec:
  imagePullSecrets:             #| This enables pulling images
  - name: mydockerhubsecret     #| from a private image registry.
  containers:
  - image: username/private:tag
    name: main

</pre>
</div>
</div>
</div>

<div id="outline-container-org7837679" class="outline-5">
<h5 id="org7837679"><span class="section-number-5">1.10.3.3</span> Passing Secret to Container</h5>
<div class="outline-text-5" id="text-1-10-3-3">
</div>
<div id="outline-container-org0b9f597" class="outline-6">
<h6 id="org0b9f597"><span class="section-number-6">1.10.3.3.1</span> As Volume</h6>
<div class="outline-text-6" id="text-1-10-3-3-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs                    # | Configured Nginx to read the cert and
      mountPath: /etc/nginx/certs/   # | key file from /etc/nginx/certs, so you
      readOnly: true                 # | need to mount the Secret volume there.
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                 # | Define the secret
    secret:                     # | volume here, referring to
      secretName: fortune-https # | the fortune-https Secret.

</pre>
</div>



<div class="figure">
<p><img src="img/k8s_secret_vol.png" alt="k8s_secret_vol.png">
</p>
</div>
</div>
</div>




<div id="outline-container-org94cfb8a" class="outline-6">
<h6 id="org94cfb8a"><span class="section-number-6">1.10.3.3.2</span> As ENV</h6>
<div class="outline-text-6" id="text-1-10-3-3-2">
<div class="org-src-container">
<pre class="src src-yaml">env:
- name: FOO_SECRET
  valueFrom:                    #| The variable should be set
    secretKeyRef:               #| from the entry of a Secret.
      name: fortune-https       # The name of the Secret holding the key
      key: foo                  # The key of the Secret to exopse
</pre>
</div>

<pre class="example">
Think twice before using environment variables to pass your Secrets to your container, because they may get exposed inadvertently.
To be safe, always use secret volumes for exposing Secrets.
</pre>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-org382bc5f" class="outline-3">
<h3 id="org382bc5f"><span class="section-number-3">1.11</span> Deployment</h3>
<div class="outline-text-3" id="text-1-11">
<p>
Deployment sits <b>on top of ReplicaSets</b> and enables declarative application updates.
</p>

<pre class="example">
The ReplicaSet(created by Deployment)'s name also contains the hash value of its pod template.
</pre>


<div class="figure">
<p><img src="img/k8s_deploy.png" alt="k8s_deploy.png">
</p>
</div>
</div>

<div id="outline-container-org77fc172" class="outline-4">
<h4 id="org77fc172"><span class="section-number-4">1.11.1</span> Updating Strategies</h4>
<div class="outline-text-4" id="text-1-11-1">
<p>
You have two ways of updating all those pods. You can do one of the following:
</p>

<ul class="org-ul">
<li><p>
Delete all existing pods first and then start the new ones (Recreate)
</p>


<div class="figure">
<p><img src="img/k8s_deploy_recreate.png" alt="k8s_deploy_recreate.png">
</p>
</div></li>
</ul>


<ul class="org-ul">
<li><p>
Start new ones and, once they're up, delete the old ones
</p>

<p>
You can do this either by adding all the new pods and then deleting all the old ones at once, or sequentially,
by adding new pods and removing old ones gradually.
</p>

<ul class="org-ul">
<li><p>
Spinning up new pods and then deleting the old ones (blue-green deployment)
</p>


<div class="figure">
<p><img src="img/k8s_deploy_blue_green.png" alt="k8s_deploy_blue_green.png">
</p>
</div></li>
</ul></li>
</ul>


<ul class="org-ul">
<li><p>
RollingUpdate
</p>

<p>
Should use this strategy only when your app can handle running both the old and new version <b>at the same time.</b>
</p></li>
</ul>



<div class="figure">
<p><img src="img/k8s_deploy_rolling.png" alt="k8s_deploy_rolling.png">
</p>
</div>
</div>
</div>



<div id="outline-container-org142808a" class="outline-4">
<h4 id="org142808a"><span class="section-number-4">1.11.2</span> Performing RU with <code>kubectl</code></h4>
<div class="outline-text-4" id="text-1-11-2">
<div class="org-src-container">
<pre class="src src-sh">kubectl rolling-update   kubia-v1      kubia-v2   --image=luksa/kubia:v2
<span style="color: #465457;">#                      </span><span style="color: #465457;">===old rc===  ===new rc===</span>
</pre>
</div>
</div>

<div id="outline-container-org42f1dd6" class="outline-5">
<h5 id="org42f1dd6"><span class="section-number-5">1.11.2.1</span> Steps Required Before RU</h5>
<div class="outline-text-5" id="text-1-11-2-1">
<ol class="org-ol">
<li>Copying old controller to new controller and changing the image in its pod template.</li>
<li>Add an additional <code>deployment</code> label in <b>both</b> origin and new contoller.</li>
<li>Modify the labels of the live pods.</li>
</ol>


<div class="figure">
<p><img src="img/k8s_deploy_cli_warmup.png" alt="k8s_deploy_cli_warmup.png">
</p>
</div>

<p>
After setting up all this, kubectl starts replacing pods by first scaling up the new controller to 1.
The controller thus creates the first v2 pod.
kubectl then scales down the old ReplicationController by 1.
</p>


<div class="figure">
<p><img src="img/k8s_deploy_cli_doing.png" alt="k8s_deploy_cli_doing.png">
</p>
</div>
</div>
</div>
</div>














<div id="outline-container-org57cf654" class="outline-4">
<h4 id="org57cf654"><span class="section-number-4">1.11.3</span> Creating Deployment</h4>
<div class="outline-text-4" id="text-1-11-3">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: apps/v1beta1        # Deployments are in the apps API group, version v1beta1.
kind: Deployment
metadata:
  name: kubia                   # There's no need to include the version info in the name of the Deployment.
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">kubectl create -f kubia-deployment-v1.yaml --record
</pre>
</div>

<pre class="example">
Be sure to include the --record command-line option when creating it. This records the command in the revision history.
</pre>
</div>
</div>


<div id="outline-container-org294a429" class="outline-4">
<h4 id="org294a429"><span class="section-number-4">1.11.4</span> Updating Deployment</h4>
<div class="outline-text-4" id="text-1-11-4">
<p>
The only thing you need to do is modify the pod template defined in the Deployment resource and
Kubernetes will take all the steps necessary to get the actual system state to what's defined in the resource.
</p>


<div class="figure">
<p><img src="img/k8s_deploy_update_image.png" alt="k8s_deploy_update_image.png">
</p>
</div>



<div class="figure">
<p><img src="img/k8s_deploy_update_process.png" alt="k8s_deploy_update_process.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org398dbb7" class="outline-4">
<h4 id="org398dbb7"><span class="section-number-4">1.11.5</span> Rolling Back</h4>
<div class="outline-text-4" id="text-1-11-5">
<p>
Deployments make it easy to roll back to the previously deployed version by telling Kubernetes to undo the last rollout of a Deployment:
</p>

<div class="org-src-container">
<pre class="src src-sh">$ kubectl rollout undo deployment kubia
deployment <span style="color: #e6db74;">"kubia"</span> rolled back
</pre>
</div>

<pre class="example">
The undo command can also be used while the rollout process is still in progress to essentially abort the rollout.
Pods already created during the roll- out process are removed and replaced with the old ones again.
</pre>

<p>
The length of the revision history is limited by the <code>revisionHistoryLimit</code> property on the Deployment resource.
</p>
</div>
</div>









<div id="outline-container-orgedc4d3e" class="outline-4">
<h4 id="orgedc4d3e"><span class="section-number-4">1.11.6</span> maxSurge/maxUnavailable</h4>
<div class="outline-text-4" id="text-1-11-6">
<div class="org-src-container">
<pre class="src src-yaml">spec:
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
</pre>
</div>

<ul class="org-ul">
<li><p>
maxSurge
</p>

<p>
Determines how many pod instances you allow to exist <b>above</b> the desired replica count configured on the Deployment. <br>
It defaults to 25%, so there can be at most 25% more pod instances than the desired count. <br>
The value can also be an absolute value.
</p></li>
<li><p>
maxUnavailable
</p>

<p>
Determines how many pod instances can be unavailable relative to the desired replica count during the update. <br>
It also defaults to 25%, so the number of available pod instances must never fall below 75% of the desired replica count. <br>
Can also specify an absolute value instead of a percentage.
</p></li>
</ul>



<div class="figure">
<p><img src="img/k8s_deploy_rollout_rate.png" alt="k8s_deploy_rollout_rate.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org70c90c1" class="outline-4">
<h4 id="org70c90c1"><span class="section-number-4">1.11.7</span> Pausing Rollout Process</h4>
<div class="outline-text-4" id="text-1-11-7">
<p>
A Deployment can also be paused during the rollout process. <br>
This <b>allows you to verify that everything is fine with the new version</b> before proceeding with the rest of the rollout.
</p>

<div class="org-src-container">
<pre class="src src-sh">$ kubectl set image deployment kubia <span style="color: #fd971f;">nodejs</span>=luksa/kubia:v4
deployment <span style="color: #e6db74;">"kubia"</span> image updated

$ kubectl rollout pause deployment kubia
deployment <span style="color: #e6db74;">"kubia"</span> paused

<span style="color: #465457;"># </span><span style="color: #465457;">A single new pod should have been created, but all original pods should also still be running.</span>
<span style="color: #465457;"># </span><span style="color: #465457;">Once the new pod is up, a part of all requests to the service will be redirected to the new pod.</span>

<span style="color: #465457;"># </span><span style="color: #465457;">Once you're confident the new version works as it should, you can resume the deployment to replace all the old pods with new ones:</span>
$ kubectl rollout resume deployment kubia
deployment <span style="color: #e6db74;">"kubia"</span> resumed
</pre>
</div>

<pre class="example">
If a Deployment is paused, the 'undo' command won't undo it until you resume the Deployment.
</pre>
</div>
</div>


<div id="outline-container-org161b30d" class="outline-4">
<h4 id="org161b30d"><span class="section-number-4">1.11.8</span> Blocking Rollout (minReadySeconds)</h4>
<div class="outline-text-4" id="text-1-11-8">
<p>
<code>minReadySeconds</code> can be used to slow down the rollout, so you could see it was indeed performing a rolling update and not replacing all the pods at once. <br>
But the main function of <code>minReadySeconds</code> is to prevent deploying malfunctioning versions, <b>not slowing down a deployment for fun.</b>
</p>

<ul class="org-ul">
<li>This property specifies how long a newly created pod should be ready before the pod is treated as available.</li>
<li><b>Until the pod is available, the rollout process will not continue.</b></li>
<li>If a new pod's readiness probe starts failing before minReadySeconds have passed, the rollout of the new version will effectively <b>be blocked.</b></li>
</ul>


<pre class="example">
With a properly configured readiness probe and a proper minReadySeconds setting,
Kubernetes would have prevented us from deploying buggy version at an earlier stage.

If only define the readiness probe without setting 'minReadySeconds' properly,
new pods are considered available immediately when the first invocation of the readiness probe succeeds.
If the readiness probe starts failing shortly after, the bad version is rolled out across all pods.
Therefore, you should set 'minReadySeconds' appropriately.
</pre>

<p>
When the rollout will never continue, the only thing to do now is abort the rollout by undoing it:
</p>

<div class="org-src-container">
<pre class="src src-sh">$ kubectl rollout undo deployment kubia
deployment <span style="color: #e6db74;">"kubia"</span> rolled back
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org1edcb27" class="outline-3">
<h3 id="org1edcb27"><span class="section-number-3">1.12</span> StatefulSet</h3>
<div class="outline-text-3" id="text-1-12">
<p>
StatefulSet makes sure pods are rescheduled in such a way that they retain their identity and state.
</p>

<p>
StatefulSets were more like ReplicaSets and not like Deployments, so they <b>don't perform a rollout when the template is modified.</b>
</p>

<pre class="example">
Starting from Kubernetes version 1.7, StatefulSets support rolling updates the same way Deployments and DaemonSets do.
See the StatefulSet's spec.updateStrategy field documentation using kubectl explain for more information.
</pre>
</div>

<div id="outline-container-orga78315c" class="outline-4">
<h4 id="orga78315c"><span class="section-number-4">1.12.1</span> 设计思想</h4>
<div class="outline-text-4" id="text-1-12-1">
</div>
<div id="outline-container-org5360d99" class="outline-5">
<h5 id="org5360d99"><span class="section-number-5">1.12.1.1</span> Stable Identity</h5>
<div class="outline-text-5" id="text-1-12-1-1">
<p>
Each pod created by a StatefulSet is assigned an ordinal index (zero-based),
which is then used to derive the pod's name and hostname, and to attach stable storage to the pod. <br>
The names of the pods are thus <b>predictable</b>.
</p>


<div class="figure">
<p><img src="img/k8s_sts_stable_id.png" alt="k8s_sts_stable_id.png">
</p>
</div>
</div>


<div id="outline-container-org1b0c3ed" class="outline-6">
<h6 id="org1b0c3ed"><span class="section-number-6">1.12.1.1.1</span> Governing Service</h6>
<div class="outline-text-6" id="text-1-12-1-1-1">
<p>
StatefulSet requires to create a corresponding <b>governing headless Service</b> that's used to provide the actual network identity to each pod. <br>
Through this Service, <b>each pod gets its own DNS entry</b>, so its peers and possibly other clients in the cluster can <b>address the pod by its hostname:</b>
</p>

<pre class="example">
a-0       .foo          .default   .svc.cluster.local
--------  ------------  ---------  ----------------------------------
pod name  service name  namespace  configurable cluster domain suffix
</pre>

<p>
Additionally, we can also use DNS to look up all the StatefulSet's pods' names by looking up <b>SRV records</b> for the <code>foo.default.svc.cluster.local</code> domain.
</p>

<pre class="example">
SRV records are used to point to hostnames and ports of servers providing a specific service.
Kubernetes creates SRV records to point to the hostnames of the pods backing a HEADLESS SERVICE.
</pre>
</div>
</div>


<div id="outline-container-orgdeaa451" class="outline-6">
<h6 id="orgdeaa451"><span class="section-number-6">1.12.1.1.2</span> Scaling</h6>
<div class="outline-text-6" id="text-1-12-1-1-2">
<ul class="org-ul">
<li>Scaling the StatefulSet creates a new pod instance with the <b>next unused ordinal index.</b></li>
<li>Scaling down a StatefulSet always removes the instances with the <b>highest ordinal index first.</b></li>
</ul>

<pre class="example">
ONLY ONE POD INSTANCE AT A TIME WHEN SCALING DOWN/UP:

Because some stateful applications don't handle rapid scale-downs nicely,
StatefulSets scale down ONLY ONE POD INSTANCE AT A TIME
(for example: distributed data store may lose data if multiple nodes go down at the same time).
For this exact reason, StatefulSets also never permit scale-down operations IF ANY OF THE INSTANCES ARE UNHEALTHY.
If an instance is unhealthy, and you scale down by one at the same time, you've effectively lost two cluster members at once.
</pre>
</div>
</div>
</div>








<div id="outline-container-org4d43daa" class="outline-5">
<h5 id="org4d43daa"><span class="section-number-5">1.12.1.2</span> Stable Dedicated Storage</h5>
<div class="outline-text-5" id="text-1-12-1-2">
<p>
Each stateful pod instance needs to use its <b>own</b> storage, if a stateful pod is rescheduled
the new instance must have the <b>same</b> storage attached to it.
</p>

<p>
StatefulSet uses volume claim <b>templates</b> to <span class="underline">stamp out</span> PersistentVolumeClaims along with each pod instance:
</p>


<div class="figure">
<p><img src="img/k8s_sts_stable_vol.png" alt="k8s_sts_stable_vol.png">
</p>
</div>

<p>
PersistentVolumeClaim remains after a scale-down:
</p>


<div class="figure">
<p><img src="img/k8s_sts_stable_vol2.png" alt="k8s_sts_stable_vol2.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org0d32194" class="outline-5">
<h5 id="org0d32194"><span class="section-number-5">1.12.1.3</span> Guarantees</h5>
<div class="outline-text-5" id="text-1-12-1-3">
<p>
A StatefulSet must guarantee <b>at-most-one semantics</b> for stateful pod instances. <br>
This means a StatefulSet must be <span class="underline">absolutely certain</span> that a pod is no longer running before it can create a replacement pod.
</p>

<p>
Kubernetes master can only know that <b>when the cluster administrator tells it so.</b> <br>
To do that, the admin needs to <b>either</b> <span class="underline">delete the pod forcibly</span> or <span class="underline">delete the whole node.</span>
</p>


<pre class="example">
UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN

If the pod's status remains UNKNOWN for more than a few minutes (this time is configurable),
the pod is marked for deletion, and will be removed as soon as the Kubelet notifies the API server that the pod's containers have terminated.
The only thing you can do is tell the API server to delete the pod without waiting for the Kubelet to confirm that the pod is no longer running:
kubectl delete po &lt;name&gt; --force --grace-period 0
</pre>
</div>
</div>
</div>


<div id="outline-container-orgce3bfd5" class="outline-4">
<h4 id="orgce3bfd5"><span class="section-number-4">1.12.2</span> Deploying App through StatefulSet</h4>
<div class="outline-text-4" id="text-1-12-2">
<p>
To deploy app, you'll need to create two (or three) different types of objects:
</p>
<ul class="org-ul">
<li><p>
PersistentVolumes for storing your data files
</p>

<p>
Need to create these <b>only if</b> the cluster doesn't support dynamic provisioning of PersistentVolumes.
</p></li>
<li>A governing Service required by the StatefulSet.</li>
<li>The StatefulSet itself.</li>
</ul>
</div>
</div>
</div>
</div>



<div id="outline-container-org67e8ec2" class="outline-2">
<h2 id="org67e8ec2"><span class="section-number-2">2</span> Internals</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org8c01686" class="outline-3">
<h3 id="org8c01686"><span class="section-number-3">2.1</span> Architecture</h3>
<div class="outline-text-3" id="text-2-1">

<div class="figure">
<p><img src="img/k8s_components.png" alt="k8s_components.png">
</p>
</div>

<ul class="org-ul">
<li>components of the control plane
<ul class="org-ul">
<li>etcd</li>
<li>api server</li>
<li>scheduler</li>
<li>controller manager</li>
</ul></li>
<li>components running on the worker nodes
<ul class="org-ul">
<li>kubelet</li>
<li>Kubernetes service proxy (kube-proxy)</li>
<li>container runtime (Docker, rkt, or others)</li>
</ul></li>
<li>add-on components
<ul class="org-ul">
<li>DNS server</li>
<li>dashboard</li>
<li>ingress controller</li>
<li>heapster</li>
<li>container network interface network plugin</li>
</ul></li>
</ul>



<p>
Kubernetes system components communicate only with the API server.
The API server is <b>the only component</b> that communicates with etcd.
</p>

<p>
Components on the worker nodes all need to run <b>on the same node</b>,
the components of the Control Plane can easily be split <b>across multiple servers.</b>
</p>
</div>


<div id="outline-container-org444b5dd" class="outline-4">
<h4 id="org444b5dd"><span class="section-number-4">2.1.1</span> etcd</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
Kubernetes stores all its data in etcd under <code>/registry</code>.
</p>
</div>

<div id="outline-container-orga473ef2" class="outline-5">
<h5 id="orga473ef2"><span class="section-number-5">2.1.1.1</span> Optimistic Concurrency Control</h5>
<div class="outline-text-5" id="text-2-1-1-1">
<img src="https://img.mukewang.com/5b5c698900010e0004970285.jpg"/>

<p>
All Kubernetes resources include a <code>metadata.resourceVersion</code> field, which clients need to pass back to the API server when updating an object. <br>
If the version doesn't match the one stored in etcd, the API server rejects the update.
</p>
</div>
</div>


<div id="outline-container-org4ed2d75" class="outline-5">
<h5 id="org4ed2d75"><span class="section-number-5">2.1.1.2</span> RAFT</h5>
<div class="outline-text-5" id="text-2-1-1-2">
<p>
<a href="http://thesecretlivesofdata.com/raft/">Understandable Distributed Consensus by Animation</a>
</p>
</div>
</div>
</div>




<div id="outline-container-orgc88c80e" class="outline-4">
<h4 id="orgc88c80e"><span class="section-number-4">2.1.2</span> API Server</h4>
<div class="outline-text-4" id="text-2-1-2">

<div class="figure">
<p><img src="img/k8s_api_server.png" alt="k8s_api_server.png">
</p>
</div>


<ul class="org-ul">
<li><p>
Authentication Plugins
</p>

<p>
Extracts the client's username, user ID, and groups the user belongs to, that are then used in the next stage (Authorization).<br>
Plugins are called <b>in turn</b> until one of them determines who is sending the request.
</p></li>
<li><p>
Authorization Plugins
</p>

<p>
Determine whether the authenticated user can perform the requested action on the requested resource.<br>
Plugins are called <b>in turn</b> until one plugin says the user can perform the action.
</p></li>
<li>Admission Control Plugins
Modify the resource (or other related resources). <br>
The resource passes through <b>all</b> plugins. <br>
When the request is <span class="underline">only trying to read data</span>, the request doesn't go through the Admission Control.</li>
</ul>
</div>


<div id="outline-container-orgc19be46" class="outline-5">
<h5 id="orgc19be46"><span class="section-number-5">2.1.2.1</span> Resource Changes Notification</h5>
<div class="outline-text-5" id="text-2-1-2-1">
<p>
API server doesn't create resources. That's what <b>controllers in the Controller Manager</b> do. <br>
Control Plane components can request <b>to be notified</b> when a resource is created, modified, or deleted.
</p>


<div class="figure">
<p><img src="img/k8s_api_server_notify.png" alt="k8s_api_server_notify.png">
</p>
</div>
</div>
</div>
</div>






<div id="outline-container-orgc78a02e" class="outline-4">
<h4 id="orgc78a02e"><span class="section-number-4">2.1.3</span> Scheduler</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
Scheduler <b>doesn't instruct the selected node</b> (or the Kubelet running on that node) to run the pod. <br>
All the Scheduler does is update the pod definition through the API server. The API server then notifies the Kubelet
(through the WATCHING MECHANISM) that the pod has been scheduled.
</p>
</div>
</div>


<div id="outline-container-org8491ee9" class="outline-4">
<h4 id="org8491ee9"><span class="section-number-4">2.1.4</span> Controller Manager</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
Actual work (create, update, delete&#x2026;) is done by controllers running inside the Controller Manager.
</p>


<div class="figure">
<p><img src="img/k8s_controller_manager.png" alt="k8s_controller_manager.png">
</p>
</div>

<p>
(Replication Manager is a controller for ReplicationController resources)
</p>
</div>
</div>


<div id="outline-container-org53d6e72" class="outline-4">
<h4 id="org53d6e72"><span class="section-number-4">2.1.5</span> Kublet</h4>
<div class="outline-text-4" id="text-2-1-5">
<p>
Kubelet is the component responsible for <b>everything</b> running on a worker node:
</p>

<ul class="org-ul">
<li>Monitors running containers and reports their status, events, and resource consumption to the API server.</li>
<li>Runs the container liveness probes, restarting containers when the probes fail.</li>
<li>Terminates containers when their Pod is deleted from the API server and notifies the server that the pod has terminated.</li>
</ul>
</div>

<div id="outline-container-org40e706e" class="outline-5">
<h5 id="org40e706e"><span class="section-number-5">2.1.5.1</span> Run Static Pods without API Server</h5>
<div class="outline-text-5" id="text-2-1-5-1">
<p>
Instead of running Kubernetes system components natively, you can put their pod manifests into the Kubelet's manifest directory
and have the Kubelet run and manage them.
</p>


<div class="figure">
<p><img src="img/k8s_kubelet.png" alt="k8s_kubelet.png">
</p>
</div>

<pre class="example">
You can also use the same method to run your custom system containers, but doing it through a DaemonSet is the recommended method.
</pre>
</div>
</div>
</div>


<div id="outline-container-orgea44939" class="outline-4">
<h4 id="orgea44939"><span class="section-number-4">2.1.6</span> Kube-Proxy</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
kube-proxy's purpose is to make sure clients can connect to the services you define through the Kubernetes API.
</p>


<div class="figure">
<p><img src="img/k8s_kubeproxy_iptables.png" alt="k8s_kubeproxy_iptables.png">
</p>
</div>
</div>
</div>



<div id="outline-container-orgf22fbf4" class="outline-4">
<h4 id="orgf22fbf4"><span class="section-number-4">2.1.7</span> Add-on</h4>
<div class="outline-text-4" id="text-2-1-7">
<p>
Add-on components are deployed as pods by submitting YAML manifests to the API server. <br>
Some of these components are deployed through a Deployment resource or a ReplicationController resource, and some through a DaemonSet.
</p>
</div>
</div>
</div>





<div id="outline-container-orgb473c80" class="outline-3">
<h3 id="orgb473c80"><span class="section-number-3">2.2</span> Cooperation between controllers</h3>
<div class="outline-text-3" id="text-2-2">

<div class="figure">
<p><img src="img/k8s_controllers_cope.png" alt="k8s_controllers_cope.png">
</p>
</div>


<div class="figure">
<p><img src="img/k8s_controllers_cope2.png" alt="k8s_controllers_cope2.png">
</p>
</div>
</div>
</div>





<div id="outline-container-org42e6cf2" class="outline-3">
<h3 id="org42e6cf2"><span class="section-number-3">2.3</span> Pod</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orgb9bb8ad" class="outline-4">
<h4 id="orgb9bb8ad"><span class="section-number-4">2.3.1</span> Infrastructure Container</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
The <span class="underline">pause container</span> is the infrastructure container that holds all the containers of a pod together. <br>
All other user-defined containers of the pod then <b>use the namespaces</b> of the pod infrastructure container.
</p>


<div class="figure">
<p><img src="img/k8x_pod_infra_container.png" alt="k8x_pod_infra_container.png">
</p>
</div>

<p>
Actual application containers may die and get restarted. When such a container starts up again, it needs to become part of the same Linux namespaces as before. <br>
If the infrastructure pod is killed in the meantime, the Kubelet <b>recreates it and all the pod’s containers.</b>
</p>
</div>
</div>

<div id="outline-container-org92f59a3" class="outline-4">
<h4 id="org92f59a3"><span class="section-number-4">2.3.2</span> Inter-pod networking</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
The network is set up by Container Network Interface (CNI) plugin, not by Kubernetes itself.
</p>
</div>

<div id="outline-container-orgda210e6" class="outline-5">
<h5 id="orgda210e6"><span class="section-number-5">2.3.2.1</span> Pods are connected NAT-less</h5>
<div class="outline-text-5" id="text-2-3-2-1">

<div class="figure">
<p><img src="img/k8s_pod_natless.png" alt="k8s_pod_natless.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org48430c9" class="outline-5">
<h5 id="org48430c9"><span class="section-number-5">2.3.2.2</span> Communication between nodes</h5>
<div class="outline-text-5" id="text-2-3-2-2">
<p>
Bridges across the nodes must use non-overlapping address ranges to prevent pods on different nodes from getting the same IP.
</p>


<div class="figure">
<p><img src="img/k8s_pod_communicate_between_nodes.png" alt="k8s_pod_communicate_between_nodes.png">
</p>
</div>

<ul class="org-ul">
<li>Node’s physical network interface needs to be con- nected to the bridge as well.</li>
<li>Routing tables on node A need to be configured so all packets destined for 10.1.2.0/24 are routed to node B.</li>
<li>Node B’s routing tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.</li>
</ul>

<pre class="example">
This works only when nodes are connected to the same network switch, without any routers in between.

It’s easier to use a Software Defined Network (SDN), which makes the nodes appear as though they’re connected to the same network switch,
regardless of the actual underlying network topology, no matter how complex it is.
Packets sent from the pod are encapsulated and sent over the network to the node running the other pod,
where they are de-encapsulated and delivered to the pod in their original form.
</pre>
</div>
</div>

<div id="outline-container-orge3f0964" class="outline-5">
<h5 id="orge3f0964"><span class="section-number-5">2.3.2.3</span> Container Network Interface</h5>
<div class="outline-text-5" id="text-2-3-2-3">
<p>
To make it easier to connect containers into a network, a project called Container Network Interface (CNI) was started. <br>
The CNI allows Kubernetes to be configured to use any CNI plugin that’s out there. These plugins include:
</p>

<ul class="org-ul">
<li>Calico</li>
<li>Flannel</li>
<li>Romana</li>
<li>Weave Net</li>
</ul>

<p>
See <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">Reference</a>.
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org7a00fa5" class="outline-3">
<h3 id="org7a00fa5"><span class="section-number-3">2.4</span> High Availability</h3>
<div class="outline-text-3" id="text-2-4">

<div class="figure">
<p><img src="img/k8s_ha.png" alt="k8s_ha.png">
</p>
</div>


<div class="figure">
<p><img src="img/k8s_ha2.png" alt="k8s_ha2.png">
</p>
</div>
</div>
</div>
</div>




<div id="outline-container-orgdc6987c" class="outline-2">
<h2 id="orgdc6987c"><span class="section-number-2">3</span> Security</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orge291ad4" class="outline-3">
<h3 id="orge291ad4"><span class="section-number-3">3.1</span> Understanding Authentication</h3>
<div class="outline-text-3" id="text-3-1">
<p>
When a request is received by the API server, it goes through the list of authentication plugins, so they can each examine the request and try to determine <b>who</b> is sending the request.
</p>

<p>
The first plugin that can extract that information from the request returns the <span class="underline">username</span>, <span class="underline">user ID</span>, and the <span class="underline">groups</span> the client belongs to back to the API server core. <br>
The API server stops invoking the remaining authentication plugins and continues onto the authorization phase.
</p>

<p>
Several authentication plugins are available. They obtain the identity of the client using the following methods:
</p>

<ol class="org-ol">
<li>From the client certificate</li>
<li>From an authentication token passed in an HTTP header</li>
<li>Basic HTTP authentication</li>
</ol>
</div>

<div id="outline-container-orgbaa7f03" class="outline-4">
<h4 id="orgbaa7f03"><span class="section-number-4">3.1.1</span> Users</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
Kubernetes distinguishes between two kinds of clients connecting to the API server:
</p>

<ol class="org-ol">
<li>Actual humans (users)</li>
<li>Pods (more specifically, applications running inside them)</li>
</ol>

<p>
Users are meant to be managed by an <b>external system</b>, such as a Single Sign On (SSO) system, <br>
but the pods use a mechanism called <b>service accounts</b>, which are created and stored in the cluster as ServiceAccount resources.
</p>

<p>
In contrast, no resource represents user accounts, which means you can’t create, update, or delete users through the API server.
</p>
</div>
</div>


<div id="outline-container-org6a7f9bc" class="outline-4">
<h4 id="org6a7f9bc"><span class="section-number-4">3.1.2</span> Group</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
Both human users and ServiceAccounts can belong to one or more groups.
</p>


<div class="figure">
<p><img src="img/k8s_group.png" alt="k8s_group.png">
</p>
</div>
</div>
</div>






<div id="outline-container-orgac7169a" class="outline-4">
<h4 id="orgac7169a"><span class="section-number-4">3.1.3</span> Service Account</h4>
<div class="outline-text-4" id="text-3-1-3">
<p>
ServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are scoped to individual namespaces.
</p>

<p>
<b>Each pod is associated with exactly one ServiceAccount</b>, but multiple pods can use the same ServiceAccount:
</p>


<div class="figure">
<p><img src="img/k8s_sa.png" alt="k8s_sa.png">
</p>
</div>

<pre class="example">
When a request bearing the authentication token is received by the API server,
the server uses the token to authenticate the client sending the request and then
determines whether or not the related ServiceAccount is allowed to perform the requested operation.
The API server obtains this information from the system-wide authorization plugin configured by the cluster administrator.
One of the available authorization plugins is the role-based access control (RBAC) plugin.
</pre>


<div class="figure">
<p><img src="img/k8s_sa_inspect.png" alt="k8s_sa_inspect.png">
</p>
</div>

<pre class="example">
The authentication tokens used in ServiceAccounts are JWT tokens.
</pre>
</div>

<div id="outline-container-org156db4a" class="outline-5">
<h5 id="org156db4a"><span class="section-number-5">3.1.3.1</span> Mountable Secrets</h5>
<div class="outline-text-5" id="text-3-1-3-1">
<p>
By default, a pod can mount any Secret it wants. But the pod’s ServiceAccount can be configured to only allow the pod to mount Secrets that are listed as mountable Secrets on the ServiceAccount. <br>
To enable this feature, the ServiceAccount must contain the following annotation: <span class="underline">kubernetes.io/enforce-mountable-secrets="true"</span>.
</p>
</div>
</div>


<div id="outline-container-orge3324b2" class="outline-5">
<h5 id="orge3324b2"><span class="section-number-5">3.1.3.2</span> Image Pull Secrets</h5>
<div class="outline-text-5" id="text-3-1-3-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
- name: my-dockerhub-secret
</pre>
</div>

<p>
Adding image pull Secrets to a ServiceAccount <b>saves you from having to add them to each pod individually.</b>
</p>
</div>
</div>


<div id="outline-container-org3577493" class="outline-5">
<h5 id="org3577493"><span class="section-number-5">3.1.3.3</span> Assigning ServiceAccount to Pod</h5>
<div class="outline-text-5" id="text-3-1-3-3">
<pre class="example">
A pod’s ServiceAccount must be set when creating the pod. It can’t be changed later.
</pre>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: curl-custom-sa
spec:
  serviceAccountName: foo       # uses customized service account instead of the default
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador
    image: luksa/kubectl-proxy:1.6.2
</pre>
</div>

<pre class="example">
When your cluster isn't using proper authorization, creating and using additional ServiceAccounts doesn't make much sense,
since even the default ServiceAccount is allowed to do anything.
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-org7c5a809" class="outline-3">
<h3 id="org7c5a809"><span class="section-number-3">3.2</span> RBAC</h3>
<div class="outline-text-3" id="text-3-2">
<p>
RBAC prevents unauthorized users from viewing or modifying the cluster state. <br>
<b>The default ServiceAccount isn't allowed to view cluster state</b>, let alone modify it in any way.
</p>

<p>
A subject (which may be a human, a ServiceAccount, or a group of users or ServiceAccounts) is <b>associated with one or more roles</b> and
each role is allowed to perform certain verbs (actions) on certain resources.
</p>

<p>
The RBAC authorization rules are configured through <span class="underline">four resources</span>, which can be grouped into <span class="underline">two groups</span>:
</p>

<ol class="org-ol">
<li><p>
Roles and ClusterRoles
</p>

<p>
Specify <b>which verbs</b> can be performed on <b>which resources</b>.
</p></li>
<li><p>
RoleBindings and ClusterRoleBindings
</p>

<p>
<b>Bind</b> the above roles to specific users, groups, or ServiceAccounts.
</p></li>
</ol>

<p>
Roles define <b>what</b> can be done, while bindings define <b>who</b> can do it:
</p>


<div class="figure">
<p><img src="img/k8s_rbac_roles.png" alt="k8s_rbac_roles.png">
</p>
</div>

<p>
Role and RoleBinding are namespaced resources, whereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not namespaced). <br>
Although RoleBindings are namespaced, they can also reference ClusterRoles: <br>
</p>


<div class="figure">
<p><img src="img/k8s_rbac_roles2.png" alt="k8s_rbac_roles2.png">
</p>
</div>

<pre class="example">
You can create a RoleBinding and have it reference a ClusterRole when you WANT TO ENABLE ACCESS TO NAMESPACED RESOURCES,
you can't use the same approach for cluster-level (non-namespaced) resources.
RoleBinding can’t grant access to cluster-level resources, even if it references a ClusterRoleBinding.
</pre>
</div>

<div id="outline-container-org8aede0b" class="outline-4">
<h4 id="org8aede0b"><span class="section-number-4">3.2.1</span> Role/RoleBinding</h4>
<div class="outline-text-4" id="text-3-2-1">
</div>
<div id="outline-container-org16139f1" class="outline-5">
<h5 id="org16139f1"><span class="section-number-5">3.2.1.1</span> Create Role</h5>
<div class="outline-text-5" id="text-3-2-1-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: foo                # if namespace is omitted, the current namespace is used
  name: service-reader
rules:
- apiGroups: [""]               # core apiGroup, which has no name - hence the ""
  verbs: ["get", "list"]        # getting individual Services (by name) and listing all of them is allowed.
  resources: ["services"]       # plural name must be used!
  # resourceNames: []             # limit access only to specific Service instances by specifying their names
</pre>
</div>

<pre class="example">
The plural form must be used when specifying resources.
</pre>
</div>
</div>

<div id="outline-container-org377b2c0" class="outline-5">
<h5 id="org377b2c0"><span class="section-number-5">3.2.1.2</span> Binding Role to ServiceAccount</h5>
<div class="outline-text-5" id="text-3-2-1-2">
<p>
A Role defines what actions can be performed, but it doesn't specify who can perform them. <br>
To do that, you must bind the Role to a subject, which can be <span class="underline">a user, a ServiceAccount, or a group (of users or ServiceAccounts).</span>
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo
<span style="color: #465457;"># </span><span style="color: #465457;">you can check yaml definition by run: kubectl get rolebinding test -n foo -o yaml</span>
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_role_rolebinding.png" alt="k8s_role_rolebinding.png">
</p>
</div>

<pre class="example">
To bind a Role to a user instead of a ServiceAccount, use the --user argument to specify the username.
To bind it to a group, use --group.
</pre>
</div>


<div id="outline-container-orgf34c60d" class="outline-6">
<h6 id="orgf34c60d"><span class="section-number-6">3.2.1.2.1</span> Including ServiceAccounts from other namespaces</h6>
<div class="outline-text-6" id="text-3-2-1-2-1">
<p>
RoleBinding can add the other pod's ServiceAccount, <b>even though it's in a different namespace</b>:
</p>


<div class="figure">
<p><img src="img/k8s_role_rolebinding2.png" alt="k8s_role_rolebinding2.png">
</p>
</div>
</div>
</div>
</div>
</div>



<div id="outline-container-orgba62632" class="outline-4">
<h4 id="orgba62632"><span class="section-number-4">3.2.2</span> ClusterRole/ClusterRoleBinding</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Regular Role only allows access to resources in the same namespace the Role is in,
and can't grant access to non-resource URLs (/healthz for example), <b>but ClusterRoles can.</b>
</p>

<p>
ClusterRole is a cluster-level resource for allowing access to non-namespaced resources or non-resource URLs or
used as a <b>common</b> role to be bound inside individual namespaces, saving you from having to redefine the same role in each of them.
</p>

<pre class="example">
To grant access to cluster-level resources, you must always use a ClusterRoleBinding.
</pre>


<div class="figure">
<p><img src="img/k8s_role_cluster.png" alt="k8s_role_cluster.png">
</p>
</div>


<div class="figure">
<p><img src="img/k8s_role_cluster2.png" alt="k8s_role_cluster2.png">
</p>
</div>
</div>

<div id="outline-container-org7d23286" class="outline-5">
<h5 id="org7d23286"><span class="section-number-5">3.2.2.1</span> Access non-resource URLs</h5>
<div class="outline-text-5" id="text-3-2-2-1">
<p>
API server also exposes non-resource URLs. <br>
Access to these URLs must also be granted explicitly; otherwise the API server will reject the client’s request. <br>
Usually, this is done for you automatically through the <code>system:discovery</code> ClusterRole and the identically named ClusterRoleBinding.
</p>

<div class="org-src-container">
<pre class="src src-sh">$ kubectl get clusterrole system:discovery -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:discovery
  ...
rules:
- nonResourceURLs:              <span style="color: #465457;"># </span><span style="color: #465457;">refers to non-resource URLs</span>
  - /api
  - /api/*
  - /apis
  - /apis/*
  - /healthz
  - /swaggerapi
  - /swaggerapi/*
  - /version
</pre>
</div>
</div>
</div>

<div id="outline-container-org1ba0bd2" class="outline-5">
<h5 id="org1ba0bd2"><span class="section-number-5">3.2.2.2</span> Access namespaced resources</h5>
<div class="outline-text-5" id="text-3-2-2-2">
<p>
If you create a ClusterRoleBinding and reference the ClusterRole in it, the subjects listed in the binding can view the specified resources across all namespaces. <br>
If, on the other hand, you create a RoleBinding, the subjects listed in the binding <b>can only view resources in the namespace of the RoleBinding.</b>
</p>


<div class="figure">
<p><img src="img/k8s_role_cluster_binding.png" alt="k8s_role_cluster_binding.png">
</p>
</div>


<div class="figure">
<p><img src="img/k8s_role_cluster_binding2.png" alt="k8s_role_cluster_binding2.png">
</p>
</div>
</div>
</div>
</div>





<div id="outline-container-org2e7e974" class="outline-4">
<h4 id="org2e7e974"><span class="section-number-4">3.2.3</span> Combinations of role/binding types</h4>
<div class="outline-text-4" id="text-3-2-3">

<div class="figure">
<p><img src="img/k8s_role_combinations.png" alt="k8s_role_combinations.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org041a750" class="outline-4">
<h4 id="org041a750"><span class="section-number-4">3.2.4</span> Default CRs/CRBs</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
The most important roles are the <code>view</code>, <code>edit</code>, <code>admin</code>, and <code>cluster-admin</code> ClusterRoles. <br>
They’re meant to be bound to ServiceAccounts used by user-defined pods.
</p>

<ul class="org-ul">
<li><p>
view
</p>

<p>
Allowing read-only access to resources in a namespace, <span class="underline">except for Roles, RoleBindings, and Secrets.</span>
</p></li>
<li><p>
edit
</p>

<p>
Allows you to modify resources in a namespace, but also <span class="underline">allows both reading and modifying Secrets.</span> <br>
It doesn’t, however, allow viewing or modifying Roles or RoleBindings (to <b>prevent privilege escalation</b>).
</p></li>
<li><p>
admin
</p>

<p>
Complete control of the resources (<b>except ResourceQuotas and Namespace</b>) in a namespace. <br>
The main difference between the edit and the admin ClusterRoles is in the ability to view and modify Roles and RoleBindings in the namespace.
</p></li>
<li><p>
cluster-admin
</p>

<p>
Complete control of the Kubernetes cluster.
</p></li>
</ul>
</div>
</div>
</div>








<div id="outline-container-orgda71eee" class="outline-3">
<h3 id="orgda71eee"><span class="section-number-3">3.3</span> Use Node's namespaces</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-orgf9a791c" class="outline-4">
<h4 id="orgf9a791c"><span class="section-number-4">3.3.1</span> Use node’s network namespace</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
Certain pods (usually system pods) need to operate in the host’s default namespaces, allowing them to see and manipulate node-level resources and devices.
</p>


<div class="figure">
<p><img src="img/k8s_use_node_nw_ns.png" alt="k8s_use_node_nw_ns.png">
</p>
</div>


<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-network
spec:
  hostNetwork: true             # Using the host node’s network namespace
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
</pre>
</div>
</div>

<div id="outline-container-org8bc4232" class="outline-5">
<h5 id="org8bc4232"><span class="section-number-5">3.3.1.1</span> Use host port (not in host's nw ns)</h5>
<div class="outline-text-5" id="text-3-3-1-1">
<p>
Bind to a port in the node’s default namespace, but <span class="underline">still have their own network namespace</span>. <br>
This is done by using the <code>hostPort</code> property in one of the container’s ports defined in the <code>spec.containers.ports</code> field.
</p>

<p>
Don’t confuse pods using <code>hostPort</code> with pods exposed through a NodePort service. They’re two different things:
</p>


<div class="figure">
<p><img src="img/k8s_hostport_vs_nodeport.png" alt="k8s_hostport_vs_nodeport.png">
</p>
</div>


<p>
Because two processes can’t bind to the same host port. The Scheduler takes this into account when scheduling pods, <br>
so it <b>doesn’t schedule multiple pods to the same node</b>. <br>
If you have three nodes and want to deploy four pod replicas, only three will be scheduled (_one pod will remain Pending_):
</p>


<div class="figure">
<p><img src="img/k8s_hostport_limit.png" alt="k8s_hostport_limit.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: kubia-hostport
spec:
  containers:
  - image: luksa/kubia
    name: kubia
    ports:
    - containerPort: 8080       # The container can be reached on port 8080 of the pod’s IP.
      hostPort: 9000            # It can also be reached on port 9000 of the node it’s deployed on.
      protocol: TCP
</pre>
</div>

<pre class="example">
The hostPort feature is primarily used for exposing system services, which are deployed to every node using DaemonSets.
People also used it to ensure two replicas of the same pod were never scheduled to the same node.
</pre>
</div>
</div>
</div>


<div id="outline-container-org68b5fff" class="outline-4">
<h4 id="org68b5fff"><span class="section-number-4">3.3.2</span> Use node's PID/IPC namespaces</h4>
<div class="outline-text-4" id="text-3-3-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-with-host-pid-and-ipc
spec:
  hostPID: true                 # You want the pod to use the host’s PID namespace.
  hostIPC: true                 # You also want the pod to use the host’s IPC namespace.
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org31523a4" class="outline-3">
<h3 id="org31523a4"><span class="section-number-3">3.4</span> Container security context</h3>
<div class="outline-text-3" id="text-3-4">
</div>
<div id="outline-container-org32937ff" class="outline-4">
<h4 id="org32937ff"><span class="section-number-4">3.4.1</span> Run container as specific user</h4>
<div class="outline-text-4" id="text-3-4-1">
<pre class="example">
What user the container runs as is specified in the container image.
In a Dockerfile, this is done using the USER directive.
If omitted, the container runs as root.
</pre>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-as-user-guest
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 405            # specify a user ID, not a username (id 405 corresponds to the guest user in the alpine container image).
</pre>
</div>
</div>
</div>


<div id="outline-container-org40b9435" class="outline-4">
<h4 id="org40b9435"><span class="section-number-4">3.4.2</span> Run container as non-root</h4>
<div class="outline-text-4" id="text-3-4-2">
<p>
For example, when a host directory is mounted into the container, if the process running in the container is running as root, <br>
it has full access to the mounted directory, whereas if it's running as non-root, it won't.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-run-as-non-root
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsNonRoot: true        # This container will only be allowed to run as a non-root user.
</pre>
</div>

<p>
If you deploy this pod, it gets scheduled, but <b>is not allowed to run.</b>
So if anyone tampers with your container images, they won't get far.
</p>
</div>
</div>


<div id="outline-container-org703a9d5" class="outline-4">
<h4 id="org703a9d5"><span class="section-number-4">3.4.3</span> Run container in privileged mode</h4>
<div class="outline-text-4" id="text-3-4-3">
<p>
To get full access to the node's kernel, the pod's container shall run in privileged mode.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-privileged
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      privileged: true          # This container will run in privileged mode
</pre>
</div>
</div>
</div>


<div id="outline-container-org75c583c" class="outline-4">
<h4 id="org75c583c"><span class="section-number-4">3.4.4</span> Add individual kernel capabilities</h4>
<div class="outline-text-4" id="text-3-4-4">
<p>
Instead of making a container privileged and giving it unlimited permissions,
a <b>much safer</b> method (from a security perspective) is to give it access only to the kernel features it really requires.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-add-settime-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        add:
        - SYS_TIME              # Adding the SYS_TIME capability. When time is set in container, the node's time will also be changed

</pre>
</div>
</div>
</div>


<div id="outline-container-org56677fb" class="outline-4">
<h4 id="org56677fb"><span class="section-number-4">3.4.5</span> Drop individual kernel capabilities</h4>
<div class="outline-text-4" id="text-3-4-5">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-drop-chown-capability
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      capabilities:
        drop:                   # | You’re not allowing this container
          - CHOWN               # | to change file ownership.

</pre>
</div>
</div>
</div>

<div id="outline-container-orgad8249e" class="outline-4">
<h4 id="orgad8249e"><span class="section-number-4">3.4.6</span> Prevent writing to filesystem</h4>
<div class="outline-text-4" id="text-3-4-6">
<p>
Prevent the processes running in the container from writing to the container’s filesystem, and <b>only allow</b> them to write to mounted volumes.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readonly-filesystem
spec:
  containers:
  - name: main
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      readOnlyRootFilesystem: true # This container’s filesystem can’t be written to.
    volumeMounts:
    - name: my-volume
      mountPath: /volume           # | Writing to /volume is allowed,
      readOnly: false              # | becase a volume is mounted there.
  volumes:
  - name: my-volume
    emptyDir:
</pre>
</div>
</div>
</div>


<div id="outline-container-org6eca4b9" class="outline-4">
<h4 id="org6eca4b9"><span class="section-number-4">3.4.7</span> Share volume among different users</h4>
<div class="outline-text-4" id="text-3-4-7">
<p>
Kubernetes allows you to specify supplemental groups for all the pods running in the container, allowing them to share files,
<b>regardless of the user IDs they’re running as.</b>
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-with-shared-volume-fsgroup
spec:
  securityContext:                 # | The fsGroup and supplementalGroups are defined
    fsGroup: 555                   # | in the security context at the pod level.
    supplementalGroups: [666, 777] # |
  containers:
  - name: first
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 1111           # The first container runs as user ID 1111.
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  - name: second
    image: alpine
    command: ["/bin/sleep", "999999"]
    securityContext:
      runAsUser: 2222           # The first container runs as user ID 2222.
    volumeMounts:
    - name: shared-volume
      mountPath: /volume
      readOnly: false
  volumes:
  - name: shared-volume
    emptyDir:
</pre>
</div>

<pre class="example">
fsGroup security context property is used when the process creates files IN A VOLUME.
</pre>
</div>
</div>
</div>


<div id="outline-container-org5bac8cf" class="outline-3">
<h3 id="org5bac8cf"><span class="section-number-3">3.5</span> PodSecurityPolicy</h3>
<div class="outline-text-3" id="text-3-5">
<p>
PodSecurityPolicy is a <b>cluster-level</b> resource, which defines what security-related features users can or can’t use in their pods.
</p>

<p>
When someone posts a pod resource to the API server, the <code>PodSecurityPolicy admission control plugin</code> validates the pod definition against the configured PodSecurityPolicies. <br>
If the pod conforms to the cluster’s policies, it’s accepted and stored into etcd; otherwise it’s rejected immediately.
</p>

<p>
PodSecurityPolicy resource defines things like:
</p>

<ul class="org-ul">
<li>Whether a pod can use the host’s IPC, PID, or Network namespaces</li>
<li>Which host ports a pod can bind to</li>
<li>What user IDs a container can run as</li>
<li>Whether a pod with privileged containers can be created</li>
<li>Which kernel capabilities are allowed, which are added by default and which are always dropped</li>
<li>What SELinux labels a container can use</li>
<li>Whether a container can use a writable root filesystem or not</li>
<li>Which filesystem groups the container can run as</li>
<li>Which volume types a pod can use</li>
</ul>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  hostIPC: false                # | Containers aren’t allowed to use the host’s IPC,
  hostPID: false                # | PID, or network namespace.
  hostNetwork: false            # |
  hostPorts:
  - min: 10000                  # | They can only bind to host ports 10000 to 11000 or host ports 13000 to 14000.
    max: 11000                  # |
  - min: 13000                  # |
    max: 14000                  # |
  privileged: false             # Containers cannot run in privileged mode.
  readOnlyRootFilesystem: true  # Containers are forced to run with a read-only root filesystem.
  runAsUser:                    # | Containers can run as any user and any group.
    rule: RunAsAny              # |
  fsGroup:                      # |
    rule: RunAsAny              # |
  supplementalGroups:           # |
    rule: RunAsAny              # |
  seLinux:
    rule: RunAsAny              # They can also use any SELinux groups they want.
  volumes:
  - '*'                         # All volume types can be used in pods.
</pre>
</div>

<pre class="example">
When creating pods, if ANY POLICY allows you to deploy a pod with certain features, the API server will accept your pod.
</pre>
</div>
</div>


<div id="outline-container-org4c997ea" class="outline-3">
<h3 id="org4c997ea"><span class="section-number-3">3.6</span> Inter-Pod Network Isolation</h3>
<div class="outline-text-3" id="text-3-6">
</div>
<div id="outline-container-org211b7f8" class="outline-4">
<h4 id="org211b7f8"><span class="section-number-4">3.6.1</span> Network isolation in a namespace</h4>
<div class="outline-text-4" id="text-3-6-1">
<p>
When you create this NetworkPolicy in a certain namespace, no one can connect to any pod in that namespace:
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector:                  # Empty pod selector matches all pods in the same namespace
</pre>
</div>
</div>
</div>


<div id="outline-container-org3f54a20" class="outline-4">
<h4 id="org3f54a20"><span class="section-number-4">3.6.2</span> Allow some pods in a ns to connect</h4>
<div class="outline-text-4" id="text-3-6-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:
    matchLabels:
      app: database             # This policy secures access to pods with app=database label.
  ingress:                      # | It allows incoming connections only from pods with the app=webserver label.
  - from:                       # |
    - podSelector:              # |
        matchLabels:            # |
          app: webserver        # |
    ports:
    - port: 5432                # Connections to this port are allowed.
</pre>
</div>



<div class="figure">
<p><img src="img/k8s_network_policy.png" alt="k8s_network_policy.png">
</p>
</div>

<pre class="example">
The NetworkPolicy is enforced when con- necting through a Service, AS WELL.
</pre>
</div>
</div>





<div id="outline-container-orga26fac6" class="outline-4">
<h4 id="orga26fac6"><span class="section-number-4">3.6.3</span> Isolate network between namespaces</h4>
<div class="outline-text-4" id="text-3-6-3">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:
    matchLabels:
      app: shopping-cart        # This policy applies to pods labeled as microservice=shopping-cart.
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: manning       # Only pods running in namespaces labeled as tenant=manning are allowed to access the microservice.
    ports:
    - port: 80
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_network_policy_2.png" alt="k8s_network_policy_2.png">
</p>
</div>
</div>
</div>


<div id="outline-container-org20c5e32" class="outline-4">
<h4 id="org20c5e32"><span class="section-number-4">3.6.4</span> Isolate by CIDR notation</h4>
<div class="outline-text-4" id="text-3-6-4">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ipblock-netpolicy
spec:
  podSelector:
    matchLabels:
      app: shopping-cart
  ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24    # This ingress rule only allows traffic from clients in the 192.168.1.0/24 IP block.
</pre>
</div>
</div>
</div>


<div id="outline-container-org5791726" class="outline-4">
<h4 id="org5791726"><span class="section-number-4">3.6.5</span> Limite outbound traffic</h4>
<div class="outline-text-4" id="text-3-6-5">
<div class="org-src-container">
<pre class="src src-yaml">spec:
  podSelector:
    matchLabels:
      app: webserver           # This policy applies to pods with the app=webserver label.
  egress:                      # It limits the pods' outbound traffic.
  - to:
    - podSelector:
        matchLabels:
          app: database # Webserver pods may only connect to pods with the app=database label.
</pre>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-org5141bee" class="outline-2">
<h2 id="org5141bee"><span class="section-number-2">4</span> Computation Resources</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org2f003d4" class="outline-3">
<h3 id="org2f003d4"><span class="section-number-3">4.1</span> Request resources for containers</h3>
<div class="outline-text-3" id="text-4-1">
<p>
When creating a pod, you can specify the amount of CPU and memory that a container needs (these are called <b>requests</b>) and
a hard limit on what it may consume (known as <b>limits</b>). <br>
They're specified <span class="underline">for each container</span> individually, <span class="underline">not for the pod as a whole</span>.
The pod's resource requests and limits are the <b>sum of the requests and limits of all its containers</b>.
</p>
</div>


<div id="outline-container-orgb70ce97" class="outline-4">
<h4 id="orgb70ce97"><span class="section-number-4">4.1.1</span> Create pods with resource requests</h4>
<div class="outline-text-4" id="text-4-1-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main                  # | You're specifying resource requests for the main container.
    resources:                  # |
      requests:                 # |
        cpu: 200m               # The container requests 200 millicores (that is, 1/5 of a single CPU core's time).
        memory: 10Mi            # The container also requests 10 mebibytes of memory.
</pre>
</div>

<pre class="example">
REQUESTS don't limit the amount of CPU a container can use.
</pre>
</div>

<div id="outline-container-org348089a" class="outline-5">
<h5 id="org348089a"><span class="section-number-5">4.1.1.1</span> How requests affect scheduling</h5>
<div class="outline-text-5" id="text-4-1-1-1">
<p>
By specifying resource requests, you're specifying the minimum amount of resources your pod needs.
</p>


<div class="figure">
<p><img src="img/k8s_resource_req.png" alt="k8s_resource_req.png">
</p>
</div>

<p>
The CPU requests don't only affect scheduling, they also determine how the remaining (unused) CPU time is distributed between pods:
</p>


<div class="figure">
<p><img src="img/k8s_resource_req_2.png" alt="k8s_resource_req_2.png">
</p>
</div>

<pre class="example">
If one container wants to use up as much CPU as it can, while the other one is sitting idle at a given moment,
the first container will BE ALLOWED to use the whole CPU time (minus the small amount of time used by the second container, if any).
It makes sense to use all the available CPU if no one else is using it.
As soon as the second container needs CPU time, it will get it and the first container will be THROTTLED BACK.
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-org6ebc020" class="outline-3">
<h3 id="org6ebc020"><span class="section-number-3">4.2</span> Limit resources for containers</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Without limiting memory, a container (or a pod) running on a worker node may eat up all the available memory and
affect all other pods on the node and any new pods scheduled to the node.
</p>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1                  # This container will be allowed to use at most 1 CPU core.
        memory: 20Mi            # The container will be allowed to use up to 20 mebibytes of memory.
</pre>
</div>

<pre class="example">
If you haven’t specified any resource requests, they’ll be set to the SAME VALUES AS THE RESOURCE LIMITS.
</pre>
</div>

<div id="outline-container-orgced9fff" class="outline-4">
<h4 id="orgced9fff"><span class="section-number-4">4.2.1</span> Exceeding limits</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Unlike resource requests, resource limits <b>aren't constrained</b> by the node's allocatable resource amounts. <br>
The sum of all limits of all the pods on a node is <b>allowed to exceed 100%</b> of the node's capacity.
</p>

<p>
When 100% of the node's resources are used up, certain containers will <span class="underline">need to be killed</span>.
</p>


<div class="figure">
<p><img src="img/k8s_resource_limit_mem.png" alt="k8s_resource_limit_mem.png">
</p>
</div>


<ul class="org-ul">
<li><p>
For CPU
</p>

<p>
When a CPU limit is set for a container, the process isn’t given more CPU time than the config- ured limit.
</p></li>
<li><p>
For Memory
</p>

<p>
When a process tries to allocate memory over its limit, the process is killed ( <span class="underline">the container is OOMKilled</span> ).
</p></li>
</ul>

<pre class="example">
If the pod is killed and its restart policy is set to Always or OnFailure, the process is restarted immediately.
But if it keeps going over the memory limit and getting killed, Kubernetes will begin restarting it WITH INCREASING DELAYS between restarts.
You’ll see a CrashLoopBackOff status in that case.

The CrashLoopBackOff status doesn't mean the Kubelet has given up.
It means that after each crash, the Kubelet is INCREASING THE TIME PERIOD BEFORE restarting the container.
After the first crash, it restarts the container immediately and then, if it crashes again, waits for 10 seconds before restarting it again.
On subsequent crashes, this delay is then increased EXPONENTIALLY to 20, 40, 80, and 160 seconds, and FINALLY limited to 300 seconds.
Once the interval hits the 300-second limit, the Kubelet keeps restarting the container INDEFINITELY EVERY FIVE MINUTES until the pod either stops crashing or is deleted.
</pre>

<pre class="example">
Containers always see the node's memory and cpu, NOT the container's.
You get the configured CPU limit by reading the following files:
- /sys/fs/cgroup/cpu/cpu.cfs_quota_us
- /sys/fs/cgroup/cpu/cpu.cfs_period_us
</pre>
</div>
</div>
</div>



<div id="outline-container-orgacdbb79" class="outline-3">
<h3 id="orgacdbb79"><span class="section-number-3">4.3</span> Pod QoS classes</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Kubernetes makes priority by categorizing pods into three Quality of Service (QoS) classes:
</p>
<ul class="org-ul">
<li>BestEffort (the lowest priority)</li>
<li>Burstable</li>
<li>Guaranteed (the highest)</li>
</ul>

<p>
QoS class is not assignable to pods through a separate field in the manifest.
It is <b>derived</b> from the combination of resource requests and limits for the pod's containers.
</p>


<div class="figure">
<p><img src="img/k8s_qos_class.png" alt="k8s_qos_class.png">
</p>
</div>

<pre class="example">
QoS classes are a PROPERTY OF PODS, not containers.
</pre>
</div>

<div id="outline-container-orgfc8c8fe" class="outline-4">
<h4 id="orgfc8c8fe"><span class="section-number-4">4.3.1</span> Kill Pod according to QoS</h4>
<div class="outline-text-4" id="text-4-3-1">

<div class="figure">
<p><img src="img/k8s_qos_kill.png" alt="k8s_qos_kill.png">
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org2f54c66" class="outline-3">
<h3 id="org2f54c66"><span class="section-number-3">4.4</span> Set default requests/limits for pods per ns</h3>
<div class="outline-text-3" id="text-4-4">

<div class="figure">
<p><img src="img/k8s_limit_range.png" alt="k8s_limit_range.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod                   # Specifies the limits for a pod AS A WHOLE
    min:
      cpu: 50m                  # | Minimum CPU and memory all the pod's containers can request in total
      memory: 5Mi               # |
    max:
      cpu: 1                    # | Maximum CPU and memory all the pod's containers can request (and limit)
      memory: 1Gi               # |
  - type: Container             # The container limits are specified below this line.
    defaultRequest:             # Default requests for CPU and memory that will be applied to containers that don't specify them explicitly
      cpu: 100m
      memory: 10Mi
    default:                    # | Default limits for containers that don't specify them
      cpu: 200m                 # |
      memory: 100Mi
    min:                        # | Minimum and maximum requests/limits that a container can have
      cpu: 50m                  # |
      memory: 5Mi               # |
    max:                        # |
      cpu: 1                    # |
      memory: 1Gi
    maxLimitRequestRatio:       # | Maximum ratio between the limit and request for each resource
      cpu: 4                    # |
      memory: 10                # |
  - type: PersistentVolumeClaim # A LimitRange can also set the minimum and maximum amount of storage a PVC can request.
    min:
      storage: 1Gi
    max:
      storage: 10Gi
</pre>
</div>

<pre class="example">
LimitRange apply to the SUM of all the pod's containers' requests and limits.
</pre>
</div>
</div>



<div id="outline-container-org6d6568a" class="outline-3">
<h3 id="org6d6568a"><span class="section-number-3">4.5</span> Limit total resources available in ns</h3>
<div class="outline-text-3" id="text-4-5">
<p>
ResourceQuota Admission Control plugin checks whether the pod being created would cause the configured ResourceQuota to be exceeded. <br>
If that's the case, the pod's creation is rejected.
</p>

<p>
A ResourceQuota limits the amount of computational resources the pods and the amount of storage PersistentVolumeClaims in a namespace can consume.
</p>


<div class="figure">
<p><img src="img/k8s_resource_quota.png" alt="k8s_resource_quota.png">
</p>
</div>


<pre class="example">
If ResourceQuota is created, LimitRange shall be also created ALONG WITH IT.
</pre>
</div>

<div id="outline-container-org60ab125" class="outline-4">
<h4 id="org60ab125"><span class="section-number-4">4.5.1</span> For CPU and Memory</h4>
<div class="outline-text-4" id="text-4-5-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    requests.cpu: 400m
    requests.memory: 200Mi
    limits.cpu: 600m
    limits.memory: 500Mi
</pre>
</div>

<pre class="example">
When a quota for a specific resource (CPU or memory) is configured (request or limit),
pods need to have the request or limit (respectively) set for that same resource;
otherwise the API server will not accept the pod.
That's why having a LimitRange with defaults for those resources can make life a bit easier for people creating pods.
</pre>
</div>
</div>


<div id="outline-container-orga19ba8f" class="outline-4">
<h4 id="orga19ba8f"><span class="section-number-4">4.5.2</span> For persistent storage</h4>
<div class="outline-text-4" id="text-4-5-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc8c5597" class="outline-4">
<h4 id="orgc8c5597"><span class="section-number-4">4.5.3</span> For number of objects</h4>
<div class="outline-text-4" id="text-4-5-3">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 5
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf4f9c23" class="outline-4">
<h4 id="orgf4f9c23"><span class="section-number-4">4.5.4</span> For pod states and/or QoS classes</h4>
<div class="outline-text-4" id="text-4-5-4">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:
  - BestEffort
  - NotTerminating
  hard:
    pods: 4
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-org1d1661e" class="outline-3">
<h3 id="org1d1661e"><span class="section-number-3">4.6</span> Resource usage Monitoring</h3>
<div class="outline-text-3" id="text-4-6">
<p>
Kubelet itself contains an agent called <span class="underline">cAdvisor</span>, which performs the basic collection of resource consumption data for both
<b>individual containers</b> running on the node and <b>the node</b> as a whole. <br>
Gathering those statistics <b>centrally</b> for the whole cluster requires you to run an additional component called <span class="underline">Heapster</span>.
</p>


<div class="figure">
<p><img src="img/k8s_cadvisor_heapster.png" alt="k8s_cadvisor_heapster.png">
</p>
</div>

<p>
It's Heapster that connects to all the cAdvisors, and it's the cAdvisors that collect the container and node usage data without
having to talk to the processes running inside the pods' containers.
</p>

<p>
Running Heapster in your cluster makes it possible to obtain resource usages for nodes and individual pods through the <code>kubectl top</code> command:
(<code>kubectl top</code> command gets the metrics from Heapster, which aggregates the data over a few minutes and doesn't expose it immediately)
</p>

<div class="org-src-container">
<pre class="src src-sh">$ kubectl top node
NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
minikube   170m         8%        556Mi           27%
<span style="color: #465457;"># </span><span style="color: #465457;">This shows the actual (current) CPU and memory usage of ALL THE PODS running on the node</span>


<span style="color: #465457;"># </span><span style="color: #465457;">Displaying CPU and memory usage for individual pods</span>
$ kubectl top pod --all-namespaces
NAMESPACE      NAME                          CPU(cores)   MEMORY(bytes)
kube-system    influxdb-grafana-2r2w9        1m           32Mi
kube-system    heapster-40j6d                0m           18Mi

<span style="color: #465457;"># </span><span style="color: #465457;">To see resource usages across individual containers</span>
kubectl top pod --containers
</pre>
</div>
</div>

<div id="outline-container-org51f8a5d" class="outline-4">
<h4 id="org51f8a5d"><span class="section-number-4">4.6.1</span> Analyzing resource usage with Grafana</h4>
<div class="outline-text-4" id="text-4-6-1">
<div class="org-src-container">
<pre class="src src-sh">$ kubectl cluster-info
monitoring-grafana is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
</pre>
</div>

<p>
or:
</p>

<div class="org-src-container">
<pre class="src src-sh">$ minikube service monitoring-grafana -n kube-system
Opening kubernetes service kube-system/monitoring-grafana<span style="color: #f92672; font-weight: bold;"> in</span> default browser...
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_grafana_example.png" alt="k8s_grafana_example.png">
</p>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-orgd9741fa" class="outline-2">
<h2 id="orgd9741fa"><span class="section-number-2">5</span> AutoScaling</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org8b00482" class="outline-3">
<h3 id="org8b00482"><span class="section-number-3">5.1</span> Horizontal pod autoscaling</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Horizontal pod autoscaling is the automatic scaling of the number of pod replicas managed by a controller. <br>
It's performed by the <span class="underline">Horizontal controller</span>, which is enabled and configured by creating a <span class="underline">HorizontalPodAutoscaler</span> (HPA) resource.
</p>

<pre class="example">
Pods' containers' memory consumption isn't a good metric for autoscaling.
</pre>
</div>

<div id="outline-container-org5a10881" class="outline-4">
<h4 id="org5a10881"><span class="section-number-4">5.1.1</span> Autoscaling process</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
The autoscaling process can be split into <b>three steps</b>:
</p>
<ol class="org-ol">
<li>Obtain metrics of all the pods managed by the scaled resource object.</li>
<li>Calculate the number of pods required to bring the metrics to (or close to) the specified target value.</li>
<li>Update the replicas field of the scaled resource.</li>
</ol>



<ul class="org-ul">
<li><p>
Obtain pod metrics
</p>

<p>
<img src="img/k8s_auto_scaling_metrics.png" alt="k8s_auto_scaling_metrics.png">
The horizontal pod autoscaler controller gets the metrics of all the pods by querying Heapster through REST calls.
</p></li>
<li><p>
Calculate required number of pods
</p>


<div class="figure">
<p><img src="img/k8s_auto_scaling_calc.png" alt="k8s_auto_scaling_calc.png">
</p>
</div></li>
<li><p>
Update desired replica count
</p>

<p>
<img src="img/k8s_auto_scaling_update.png" alt="k8s_auto_scaling_update.png">
Currently the only objects you can attach an Autoscaler to:
</p>
<ol class="org-ol">
<li>Deployment</li>
<li>ReplicaSet</li>
<li>ReplicationController</li>
<li>StatefulSet</li>
</ol></li>
</ul>
</div>

<div id="outline-container-org1b1cc1e" class="outline-5">
<h5 id="org1b1cc1e"><span class="section-number-5">5.1.1.1</span> Whole picture</h5>
<div class="outline-text-5" id="text-5-1-1-1">

<div class="figure">
<p><img src="img/k8s_auto_scaling_whole.png" alt="k8s_auto_scaling_whole.png">
</p>
</div>
</div>
</div>





<div id="outline-container-orge7c658e" class="outline-5">
<h5 id="orge7c658e"><span class="section-number-5">5.1.1.2</span> CPU utilization</h5>
<div class="outline-text-5" id="text-5-1-1-2">
<p>
As far as the Autoscaler is concerned, only the pod's guaranteed CPU amount (the CPU requests) is concerned when determining the CPU utilization of a pod.
</p>

<p>
The Autoscaler <b>compares the pod's actual CPU consumption and its CPU requests</b>,
which means the pods you're autoscaling <span class="underline">need to have CPU requests set</span>.
</p>
</div>
</div>


<div id="outline-container-orgd169309" class="outline-5">
<h5 id="orgd169309"><span class="section-number-5">5.1.1.3</span> Maximum rate of scaling</h5>
<div class="outline-text-5" id="text-5-1-1-3">
<p>
The autoscaler will <b>at most</b> double the number of replicas in a single operation, <span class="underline">if more than two current replicas exist</span>. <br>
<span class="underline">If only one or two exist</span>, it will scale up to a maximum of four replicas in a single step.
</p>

<p>
A scale-up will occur only if no rescaling event occurred <b>in the last three minutes</b>. <br>
A scale-down event is performed <b>every five minutes</b>.
</p>
</div>
</div>
</div>

<div id="outline-container-org6b4127e" class="outline-4">
<h4 id="org6b4127e"><span class="section-number-4">5.1.2</span> Create HorizontalPodAutoscaler</h4>
<div class="outline-text-4" id="text-5-1-2">
<div class="org-src-container">
<pre class="src src-sh">kubectl autoscale deployment &lt;name&gt; --cpu-percent=30 --min=1 --max=5
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-orgc613aac" class="outline-3">
<h3 id="orgc613aac"><span class="section-number-3">5.2</span> Horizontal node autoscaling</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-orgc242f25" class="outline-4">
<h4 id="orgc242f25"><span class="section-number-4">5.2.1</span> ClusterAutoscaler</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
Cluster Autoscaler looks out for such pods and asks the <b>cloud provider</b> to start up an additional node.
</p>
</div>

<div id="outline-container-org65454bb" class="outline-5">
<h5 id="org65454bb"><span class="section-number-5">5.2.1.1</span> Scaling up case</h5>
<div class="outline-text-5" id="text-5-2-1-1">

<div class="figure">
<p><img src="img/k8s_cluster_auto_scaler.png" alt="k8s_cluster_auto_scaler.png">
</p>
</div>


<p>
When the new node starts up, the Kubelet on that node contacts the API server and registers the node by creating a Node resource. <br>
From then on, the node is part of the Kubernetes cluster and pods can be scheduled to it.
</p>
</div>
</div>

<div id="outline-container-orgf672faf" class="outline-5">
<h5 id="orgf672faf"><span class="section-number-5">5.2.1.2</span> Scaling down case</h5>
<div class="outline-text-5" id="text-5-2-1-2">
<p>
A node will only be returned to the cloud provider if the Cluster Autoscaler knows
the pods running on the node will be rescheduled to other nodes.
</p>

<p>
When a node is selected to be shut down, the node is first <span class="underline">marked as unschedulable</span> and then all the pods running on the node are <span class="underline">evicted</span>. <br>
Because all those pods belong to ReplicaSets or other controllers, their replacements are created and scheduled to the remaining nodes
(that's why the node that's being shut down is first marked as unschedulable).
</p>
</div>
</div>
</div>


<div id="outline-container-org0c40904" class="outline-4">
<h4 id="org0c40904"><span class="section-number-4">5.2.2</span> Manually cordon and drain nodes</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
A node can also be marked as <code>unschedulable</code> and <code>drained</code> manually.
</p>

<p>
This is done with the following kubectl commands:
</p>

<ul class="org-ul">
<li><p>
<code>kubectl cordon &lt;node&gt;</code>
</p>

<p>
Marks the node as unschedulable (<b>but doesn't do anything with pods running on that node</b>).
</p></li>
<li><p>
<code>kubectl drain &lt;node&gt;</code>
</p>

<p>
Marks the node as <code>unschedulable</code> and <b>then evicts</b> all the pods from the node.
</p></li>
</ul>

<pre class="example">
In both cases, no new pods are scheduled to the node UNTIL YOU UNCORDON IT AGAIN with `kubectl uncordon &lt;node&gt;`.
</pre>
</div>
</div>


<div id="outline-container-orgdeed022" class="outline-4">
<h4 id="orgdeed022"><span class="section-number-4">5.2.3</span> PodDisruptionBudget</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
PodDisruptionBudget specifies <b>minimum number of pods</b> that need to keep running while performing scaling-down operations.
</p>

<p>
As long as PodDisruptionBudget exists, both the <code>Cluster Autoscaler</code> and the <code>kubectl drain</code> command (<b>not including auto-scaling-down</b>) will adhere to it. <br>
For example, if there were four pods altogether and <code>minAvailable</code> was set to three, the pod eviction process would evict pods <span class="underline">one by one</span>,
waiting for the evicted pod to be replaced with a new one by the ReplicaSet controller, before evicting another pod.
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org073c4a3" class="outline-2">
<h2 id="org073c4a3"><span class="section-number-2">6</span> Scheduling</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org2c66667" class="outline-3">
<h3 id="org2c66667"><span class="section-number-3">6.1</span> Node Affinity</h3>
<div class="outline-text-3" id="text-6-1">
</div>
<div id="outline-container-org2d517cf" class="outline-4">
<h4 id="org2d517cf"><span class="section-number-4">6.1.1</span> Hard</h4>
<div class="outline-text-4" id="text-6-1-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: In
            values:
            - "true"
  containers:
  - image: luksa/kubia
    name: kubia
</pre>
</div>

<pre class="example">
At this point, affinity currently only affects pod scheduling and NEVER causes a pod to be evicted from a node.
That's why all the rules right now always END WITH IgnoredDuringExecution.
</pre>
</div>
</div>


<div id="outline-container-org835d7b5" class="outline-4">
<h4 id="org835d7b5"><span class="section-number-4">6.1.2</span> Prioritizing</h4>
<div class="outline-text-4" id="text-6-1-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pref
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: pref
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          - weight: 20          # | You also prefer that your pods be scheduled to dedicated nodes,
            preference:         # | but this is four times less important than your zone preference.
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_node_affinity_prioritizing.png" alt="k8s_node_affinity_prioritizing.png">
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org224ba28" class="outline-3">
<h3 id="org224ba28"><span class="section-number-3">6.2</span> Pod Affinity</h3>
<div class="outline-text-3" id="text-6-2">
</div>
<div id="outline-container-org14d1a3b" class="outline-4">
<h4 id="org14d1a3b"><span class="section-number-4">6.2.1</span> Hard</h4>
<div class="outline-text-4" id="text-6-2-1">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: # Defining a hard requirement
          - topologyKey: kubernetes.io/hostname           # The pods of this Deployment must be deployed on the same node as the pods that match the selector.
            labelSelector:
              matchLabels:
                app: backend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
</pre>
</div>

<p>
The way <code>topologyKey</code> works is simple. The key used here isn't special. We can easily use our own <code>topologyKey</code>.
The only prerequisite is to add a corresponding label to nodes.
</p>


<div class="figure">
<p><img src="img/k8s_pod_affinity_hard.png" alt="k8s_pod_affinity_hard.png">
</p>
</div>

<pre class="example">
Scheduler also takes other pods’ pod affinity rules into account when scheduling pod:

If you now delete the backend pod, the Scheduler will still reschedule the pod to node2.
This makes sense, because otherwise if the backend pod were to be deleted by accident and rescheduled to a different node,
the frontend pods' affinity rules would be broken.
</pre>
</div>
</div>



<div id="outline-container-org8340090" class="outline-4">
<h4 id="org8340090"><span class="section-number-4">6.2.2</span> Prioritizing</h4>
<div class="outline-text-4" id="text-6-2-2">
<div class="org-src-container">
<pre class="src src-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: backend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
</pre>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-orgdb4b2d9" class="outline-2">
<h2 id="orgdb4b2d9"><span class="section-number-2">7</span> Utils</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgca2dc57" class="outline-3">
<h3 id="orgca2dc57"><span class="section-number-3">7.1</span> YAML Basic</h3>
<div class="outline-text-3" id="text-7-1">
<p>
<a href="https://plugins.jetbrains.com/plugin/9354-kubernetes-and-openshift-resource-support">IDEA Kubernetes YAML 插件</a>
</p>
</div>

<div id="outline-container-org491232b" class="outline-4">
<h4 id="org491232b"><span class="section-number-4">7.1.1</span> Key Value Pair</h4>
<div class="outline-text-4" id="text-7-1-1">
<div class="org-src-container">
<pre class="src src-yaml">fruit: apple
vegetable: carrot
liquid: water
meat: chicken
</pre>
</div>
</div>
</div>


<div id="outline-container-org732e948" class="outline-4">
<h4 id="org732e948"><span class="section-number-4">7.1.2</span> Array / List</h4>
<div class="outline-text-4" id="text-7-1-2">
<div class="org-src-container">
<pre class="src src-yaml">fruits:
  - orange
  - apple
  - banana

vegetables:
  - carrot
  - cauliflower
  - tomato
</pre>
</div>
</div>
</div>


<div id="outline-container-orge69ff24" class="outline-4">
<h4 id="orge69ff24"><span class="section-number-4">7.1.3</span> Dictionary / Map</h4>
<div class="outline-text-4" id="text-7-1-3">
<div class="org-src-container">
<pre class="src src-yaml">banana:
  calories: 105
  fat: 0.4
  carbs: 27

grape:
  calories: 62
  fat: 0.3
  carbs: 16
</pre>
</div>
</div>
</div>
</div>




<div id="outline-container-org6d6ba05" class="outline-3">
<h3 id="org6d6ba05"><span class="section-number-3">7.2</span> Bash completion</h3>
<div class="outline-text-3" id="text-7-2">
<div class="org-src-container">
<pre class="src src-sh"><span style="color: #a6e22e;">source</span> &lt;(kubectl completion bash | sed s/kubectl/&lt;your_alias&gt;/g)
</pre>
</div>
</div>
</div>


<div id="outline-container-orgf8e79ab" class="outline-3">
<h3 id="orgf8e79ab"><span class="section-number-3">7.3</span> Show all supported resources</h3>
<div class="outline-text-3" id="text-7-3">
<div class="org-src-container">
<pre class="src src-sh">kubectl get
</pre>
</div>
</div>
</div>


<div id="outline-container-orgf05f45f" class="outline-3">
<h3 id="orgf05f45f"><span class="section-number-3">7.4</span> Expose pod through Service</h3>
<div class="outline-text-3" id="text-7-4">
<div class="org-src-container">
<pre class="src src-sh">kubectl expose deployment &lt;name&gt; --port=80 --target-port=8080
</pre>
</div>
</div>
</div>


<div id="outline-container-org40478e9" class="outline-3">
<h3 id="org40478e9"><span class="section-number-3">7.5</span> Run Temporary Pod</h3>
<div class="outline-text-3" id="text-7-5">
<div class="org-src-container">
<pre class="src src-sh">kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local
</pre>
</div>

<p>
The command runs a <span class="underline">one-off</span> pod (&#x2013;restart=Never) called srvlookup,
which is attached to the console (-it) and is deleted as soon as it terminates (&#x2013;rm).
</p>
</div>
</div>


<div id="outline-container-org17b4bf7" class="outline-3">
<h3 id="org17b4bf7"><span class="section-number-3">7.6</span> Discover possible API object fields</h3>
<div class="outline-text-3" id="text-7-6">
<div class="org-src-container">
<pre class="src src-sh">kubectl explain pods
</pre>
</div>

<p>
You can then drill deeper to find out more about each attribute.
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl explain pod.spec
</pre>
</div>
</div>
</div>



<div id="outline-container-org5696165" class="outline-3">
<h3 id="org5696165"><span class="section-number-3">7.7</span> Forward local network port to port in the pod</h3>
<div class="outline-text-3" id="text-7-7">
<p>
Using port forwarding like this is an effective way to test an individual pod.
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl port-forward &lt;pod-name&gt; &lt;local-port&gt;:&lt;pod-port&gt;
</pre>
</div>
</div>
</div>


<div id="outline-container-org73b2bfd" class="outline-3">
<h3 id="org73b2bfd"><span class="section-number-3">7.8</span> Communicate with Pods through API Server</h3>
<div class="outline-text-3" id="text-7-8">
<p>
One useful feature of the API server is the ability to proxy connections directly to individual pods. <br>
If you want to perform requests against your kubia-0 pod, you hit the following URL:
</p>
<div class="org-src-container">
<pre class="src src-sh">&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/&lt;pod-name&gt;[:pod-port]/proxy/&lt;path&gt;
</pre>
</div>
</div>
</div>


<div id="outline-container-orga017b52" class="outline-3">
<h3 id="orga017b52"><span class="section-number-3">7.9</span> Connect to Services through API Server</h3>
<div class="outline-text-3" id="text-7-9">
<p>
Instead of using a piggyback pod to access the service from inside the cluster, <br>
you can use the same proxy feature provided by the API server to access the service:
</p>

<div class="org-src-container">
<pre class="src src-sh">/api/v1/namespaces/&lt;namespace&gt;/services/&lt;service-name&gt;[:service-port]/proxy/&lt;path&gt;
</pre>
</div>
</div>
</div>


<div id="outline-container-org0a6f98a" class="outline-3">
<h3 id="org0a6f98a"><span class="section-number-3">7.10</span> Obtain the log of crashed container</h3>
<div class="outline-text-3" id="text-7-10">
<p>
You can print the application’s log with kubectl logs. If your container is restarted, the kubectl logs command will show the log of the current container.
</p>

<p>
When you want to figure out why the previous container terminated, you’ll want to see those logs instead of the current container’s logs.
This can be done by using the <code>--previous</code> option:
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl logs &lt;pod&gt; --previous
</pre>
</div>
</div>
</div>


<div id="outline-container-org386b631" class="outline-3">
<h3 id="org386b631"><span class="section-number-3">7.11</span> Get IPs of all nodes</h3>
<div class="outline-text-3" id="text-7-11">
<div class="org-src-container">
<pre class="src src-sh">kubectl get nodes -o <span style="color: #fd971f;">jsonpath</span>=<span style="color: #e6db74;">'{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'</span>
</pre>
</div>

<p>
To learn more about how to use JSONPath with kubectl, refer to the [To learn more about how to use JSONPath with kubectl, refer to the <a href="http://kubernetes.io/docs/user-guide/jsonpath">documentation</a>.
</p>
</div>
</div>


<div id="outline-container-orga22c10a" class="outline-3">
<h3 id="orga22c10a"><span class="section-number-3">7.12</span> Talking to API Server</h3>
<div class="outline-text-3" id="text-7-12">
</div>
<div id="outline-container-orgb37ceb1" class="outline-4">
<h4 id="orgb37ceb1"><span class="section-number-4">7.12.1</span> kubectl proxy</h4>
<div class="outline-text-4" id="text-7-12-1">
<div class="org-src-container">
<pre class="src src-sh">kubectl proxy --port 8081
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">curl http://localhost:8081
</pre>
</div>
</div>
</div>


<div id="outline-container-orge0bdffc" class="outline-4">
<h4 id="orge0bdffc"><span class="section-number-4">7.12.2</span> From in POD</h4>
<div class="outline-text-4" id="text-7-12-2">
<div class="org-src-container">
<pre class="src src-sh"><span style="color: #a6e22e;">export</span> <span style="color: #fd971f;">CURL_CA_BUNDLE</span>=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
<span style="color: #fd971f;">TOKEN</span>=$(<span style="color: #fa8072;">cat</span> /var/run/secrets/kubernetes.io/serviceaccount/token)
<span style="color: #fd971f;">NS</span>=$(<span style="color: #fa8072;">cat</span> /var/run/secrets/kubernetes.io/serviceaccount/namespace)
curl -H <span style="color: #e6db74;">"Authorization: Bearer $TOKEN"</span> https://kubernetes
<span style="color: #465457;"># </span><span style="color: #465457;">Listing pods in the pod's own namespace</span>
curl -H <span style="color: #e6db74;">"Authorization: Bearer $TOKEN"</span> https://kubernetes/api/v1/namespaces/$<span style="color: #fd971f;">NS</span>/pods
</pre>
</div>


<div class="figure">
<p><img src="img/k8s_api_server_access.png" alt="k8s_api_server_access.png">
</p>
</div>


<ul class="org-ul">
<li><p>
Using Ambassador Container
</p>


<div class="figure">
<p><img src="img/k8s_api_server_ambassador.png" alt="k8s_api_server_ambassador.png">
</p>
</div></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgdcf3f0d" class="outline-3">
<h3 id="orgdcf3f0d"><span class="section-number-3">7.13</span> Modify Deployments and other resources</h3>
<div class="outline-text-3" id="text-7-13">
</div>
<div id="outline-container-org83ea8e9" class="outline-4">
<h4 id="org83ea8e9"><span class="section-number-4">7.13.1</span> edit</h4>
<div class="outline-text-4" id="text-7-13-1">
<p>
Opens the object's manifest in your default editor. <br>
After making changes, saving the file, and exiting the editor, the object is updated.
</p>

<p>
Example: <code>kubectl edit deployment kubia</code>
</p>
</div>
</div>


<div id="outline-container-org7ce5d8e" class="outline-4">
<h4 id="org7ce5d8e"><span class="section-number-4">7.13.2</span> patch</h4>
<div class="outline-text-4" id="text-7-13-2">
<p>
Modifies individual properties of an object. <br>
Example:
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl patch deployment kubia -p <span style="color: #e6db74;">'{"spec": {"template": {"spec": {"containers": [{"name": "nodejs", "image": "luksa/kubia:v2"}]}}}}'</span>
</pre>
</div>
</div>
</div>


<div id="outline-container-org8538e4f" class="outline-4">
<h4 id="org8538e4f"><span class="section-number-4">7.13.3</span> apply</h4>
<div class="outline-text-4" id="text-7-13-3">
<p>
Modifies the object by applying property values from a full YAML or JSON file. <br>
If the object specified in the YAML/JSON <b>doesn't exist yet, it's created.</b> <br>
The file needs to contain the <b>full</b> definition of the resource
(it can't include only the fields you want to update, as is the case with <code>kubectl patch</code>).
</p>

<p>
Example: <code>kubectl apply -f kubia-deployment-v2.yaml</code>
</p>
</div>
</div>


<div id="outline-container-org4857eaf" class="outline-4">
<h4 id="org4857eaf"><span class="section-number-4">7.13.4</span> replace</h4>
<div class="outline-text-4" id="text-7-13-4">
<p>
Replaces the object with a new one from a YAML/JSON file. <br>
In contrast to the apply command, this command <b>requires the object to exist</b>; otherwise it prints an error.
</p>

<p>
Example: <code>kubectl replace -f kubia-deployment-v2.yaml</code>
</p>
</div>
</div>

<div id="outline-container-org663b659" class="outline-4">
<h4 id="org663b659"><span class="section-number-4">7.13.5</span> set image</h4>
<div class="outline-text-4" id="text-7-13-5">
<p>
Changes the container image defined in a Pod, ReplicationController's template, Deployment, DaemonSet, Job, or ReplicaSet.
</p>

<p>
Example: <code>kubectl set image deployment kubia nodejs=luksa/kubia:v2</code>
</p>
</div>
</div>
</div>


<div id="outline-container-orgeeb1713" class="outline-3">
<h3 id="orgeeb1713"><span class="section-number-3">7.14</span> Check status of Control Plane components</h3>
<div class="outline-text-3" id="text-7-14">
<div class="org-src-container">
<pre class="src src-sh">$ kubectl get componentstatuses
NAME                 STATUS    MESSAGE            ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {<span style="color: #e6db74;">"health"</span>: <span style="color: #e6db74;">"true"</span>}

</pre>
</div>
</div>
</div>


<div id="outline-container-orgfcf0933" class="outline-3">
<h3 id="orgfcf0933"><span class="section-number-3">7.15</span> WATCHING Resources</h3>
<div class="outline-text-3" id="text-7-15">
<div class="org-src-container">
<pre class="src src-sh">kubectl get po --watch
</pre>
</div>
</div>
</div>


<div id="outline-container-orge581778" class="outline-3">
<h3 id="orge581778"><span class="section-number-3">7.16</span> Observing cluster events</h3>
<div class="outline-text-3" id="text-7-16">
<p>
Both the Control Plane components and the Kubelet <b>emit events to the API server as they perform these actions.</b>
</p>

<p>
If an event occurs multiple times, the event is displayed <b>only once</b> by <code>kubectl get events</code>,
showing when it was first seen, when it was last seen, and the number of times it occurred.
</p>

<p>
Watching events with the <code>--watch</code> option is much easier on the eyes and useful for seeing what’s happening in the cluster:
</p>

<div class="org-src-container">
<pre class="src src-sh">kubectl get events --watch
</pre>
</div>
</div>
</div>


<div id="outline-container-orgded8ba7" class="outline-3">
<h3 id="orgded8ba7"><span class="section-number-3">7.17</span> Forcely shutdown pod</h3>
<div class="outline-text-3" id="text-7-17">
<div class="org-src-container">
<pre class="src src-sh">kubectl delete po mypod --grace-period=0 --force
</pre>
</div>
</div>
</div>

<div id="outline-container-org24feee1" class="outline-3">
<h3 id="org24feee1"><span class="section-number-3">7.18</span> Copy file from/to container</h3>
<div class="outline-text-3" id="text-7-18">
<div class="org-src-container">
<pre class="src src-sh">kubectl cp foo-pod:/var/log/foo.log foo.log
kubectl cp localfile foo-pod:/etc/remotefile
</pre>
</div>
</div>
</div>


<div id="outline-container-org532765b" class="outline-3">
<h3 id="org532765b"><span class="section-number-3">7.19</span> Mounting local files into the minikube vm</h3>
<div class="outline-text-3" id="text-7-19">
<p>
You can mount your local filesystem into the Minikube VM using the <code>minikube mount</code> command and then
mount it into your containers through a <code>hostPath</code> volume.
</p>

<p>
See <a href="https://github.com/kubernetes/minikube/tree/master/docs">documentation</a>
</p>
</div>
</div>


<div id="outline-container-org50f3222" class="outline-3">
<h3 id="org50f3222"><span class="section-number-3">7.20</span> Using minikube docker daemon</h3>
<div class="outline-text-3" id="text-7-20">
<div class="org-src-container">
<pre class="src src-sh"><span style="color: #a6e22e;">eval</span> $(<span style="color: #fa8072;">minikube</span> docker-env)
</pre>
</div>
</div>
</div>

<div id="outline-container-org7d4d92a" class="outline-3">
<h3 id="org7d4d92a"><span class="section-number-3">7.21</span> Build images locally and load remotely</h3>
<div class="outline-text-3" id="text-7-21">
<div class="org-src-container">
<pre class="src src-sh">docker save &lt;image&gt; | (<span style="color: #a6e22e;">eval</span> $(<span style="color: #fa8072;">minikube</span> docker-env) &amp;&amp; docker load)
</pre>
</div>
</div>
</div>


<div id="outline-container-org6b3949f" class="outline-3">
<h3 id="org6b3949f"><span class="section-number-3">7.22</span> <code>kubectl config</code></h3>
<div class="outline-text-3" id="text-7-22">
</div>
<div id="outline-container-org318d86a" class="outline-4">
<h4 id="org318d86a"><span class="section-number-4">7.22.1</span> Location of kubeconfig file</h4>
<div class="outline-text-4" id="text-7-22-1">
<p>
You can use multiple config files and have kubectl use them all at once by specifying all of them in the <code>KUBECONFIG</code> environment variable
(separate them with a colon).
</p>
</div>
</div>

<div id="outline-container-org3cc356f" class="outline-4">
<h4 id="org3cc356f"><span class="section-number-4">7.22.2</span> Contents of kubeconfig file</h4>
<div class="outline-text-4" id="text-7-22-2">

<div class="figure">
<p><img src="img/k8s_kubeconfig_file.png" alt="k8s_kubeconfig_file.png">
</p>
</div>

<p>
The kubeconfig file consists of four sections:
</p>

<ol class="org-ol">
<li>A list of clusters</li>
<li>A list of users</li>
<li>A list of contexts</li>
<li>The name of the current context</li>
</ol>

<pre class="example">
A context ties together a cluster, a user, and the default namespace kubectl should use when performing commands.
</pre>

<p>
While there can be multiple contexts defined in the kubeconfig file,
at any given time <span class="underline">only one of them</span> is the current context.
</p>
</div>
</div>

<div id="outline-container-org31568ea" class="outline-4">
<h4 id="org31568ea"><span class="section-number-4">7.22.3</span> Use cluster from local machine</h4>
<div class="outline-text-4" id="text-7-22-3">
<div class="org-src-container">
<pre class="src src-sh">scp root@&lt;master-ip&gt;:/etc/kubernetes/admin.conf ~/.kube/config2
<span style="color: #a6e22e;">export</span> <span style="color: #fd971f;">KUBECONFIG</span>=~/.kube/config2
</pre>
</div>
</div>
</div>

<div id="outline-container-org9828cbc" class="outline-4">
<h4 id="org9828cbc"><span class="section-number-4">7.22.4</span> Name of current context</h4>
<div class="outline-text-4" id="text-7-22-4">
<div class="org-src-container">
<pre class="src src-sh">kubectl config current-context
</pre>
</div>
</div>
</div>



<div id="outline-container-org8a5ff1b" class="outline-4">
<h4 id="org8a5ff1b"><span class="section-number-4">7.22.5</span> Switch to different namespace</h4>
<div class="outline-text-4" id="text-7-22-5">
<div class="org-src-container">
<pre class="src src-sh"><span style="color: #a6e22e;">alias</span> <span style="color: #fd971f;">kcd</span>=<span style="color: #e6db74;">'kubectl config set-context $(</span><span style="color: #fa8072;">kubectl</span><span style="color: #e6db74;"> config current-context) --namespace '</span>
</pre>
</div>

<p>
You can then switch between namespaces using <code>kcd &lt;some-namespace&gt;</code>
</p>
</div>
</div>

<div id="outline-container-orged2800c" class="outline-4">
<h4 id="orged2800c"><span class="section-number-4">7.22.6</span> Switching between contexts</h4>
<div class="outline-text-4" id="text-7-22-6">
<div class="org-src-container">
<pre class="src src-sh">kubectl config use-context my-other-context
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf89aacd" class="outline-4">
<h4 id="orgf89aacd"><span class="section-number-4">7.22.7</span> List contexts and clusters</h4>
<div class="outline-text-4" id="text-7-22-7">
<div class="org-src-container">
<pre class="src src-sh">kubectl config get-contexts
kubectl config get-clusters
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org78cb38f" class="outline-3">
<h3 id="org78cb38f"><span class="section-number-3">7.23</span> Setup AWS EKS</h3>
<div class="outline-text-3" id="text-7-23">
<ol class="org-ol">
<li>prepare vpc</li>
<li>prepare subnets</li>
<li><a href="https://console.aws.amazon.com/iam/">create Amazon EKS service role</a></li>
<li><a href="https://us-east-2.console.aws.amazon.com/eks/home?region=us-east-2#/clusters">create EKS cluster</a></li>
<li><p>
update kube config
</p>

<p>
<code>aws eks --region us-east-2 update-kubeconfig --name &lt;cluster-name&gt;</code>
</p></li>
<li><a href="https://docs.aws.amazon.com/en_us/eks/latest/userguide/launch-workers.html">launch worker nodes</a></li>
</ol>
</div>
</div>
</div>
</div>
</body>
</html>